{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahtesham-Ibne-Mostafa/Escape/blob/main/ISRUC_SLEEP_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing Functions"
      ],
      "metadata": {
        "id": "ooeqcNJgP8Vo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "eZQRW5ArBw2r",
        "outputId": "2dc9d295-8a1a-491a-af2f-eb6517f75aa6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\noutput:\\n    save to $path_output/ISRUC_S3.npz:\\n        Fold_data:  [k-fold] list, each element is [N,V,T]\\n        Fold_label: [k-fold] list, each element is [N,C]\\n        Fold_len:   [k-fold] list\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import scipy.io as scio\n",
        "from os import path\n",
        "from scipy import signal\n",
        "\n",
        "path_Extracted = './data/ISRUC_S3/ExtractedChannels/'\n",
        "path_RawData   = './data/ISRUC_S3/RawData/'\n",
        "path_output    = './data/ISRUC_S3/'\n",
        "channels = ['C3_A2', 'C4_A1', 'F3_A2', 'F4_A1', 'O1_A2', 'O2_A1',\n",
        "            'LOC_A2', 'ROC_A1','X1', 'X2']\n",
        "\n",
        "\n",
        "def read_psg(path_Extracted, sub_id, channels, resample=3000):\n",
        "    psg = scio.loadmat(path.join(path_Extracted, 'subject%d.mat' % (sub_id)))\n",
        "    psg_use = []\n",
        "    for c in channels:\n",
        "        psg_use.append(\n",
        "            np.expand_dims(signal.resample(psg[c], resample, axis=-1), 1))\n",
        "    psg_use = np.concatenate(psg_use, axis=1)\n",
        "    return psg_use\n",
        "\n",
        "\n",
        "def read_label(path_RawData, sub_id, ignore=30):\n",
        "    label = []\n",
        "    with open(path.join(path_RawData, '%d/%d_1.txt' % (sub_id, sub_id))) as f:\n",
        "        s = f.readline()\n",
        "        while True:\n",
        "            a = s.replace('\\n', '')\n",
        "            label.append(int(a))\n",
        "            s = f.readline()\n",
        "            if s == '' or s == '\\n':\n",
        "                break\n",
        "    return np.array(label[:-ignore])\n",
        "\n",
        "\n",
        "'''\n",
        "output:\n",
        "    save to $path_output/ISRUC_S3.npz:\n",
        "        Fold_data:  [k-fold] list, each element is [N,V,T]\n",
        "        Fold_label: [k-fold] list, each element is [N,C]\n",
        "        Fold_len:   [k-fold] list\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "jMDO3WwqQCtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fold_label = []\n",
        "fold_psg = []\n",
        "fold_len = []\n",
        "\n",
        "for sub in range(1, 11):\n",
        "    print('Read subject', sub)\n",
        "    label = read_label(path_RawData, sub)\n",
        "    psg = read_psg(path_Extracted, sub, channels)\n",
        "    print('Subject', sub, ':', label.shape, psg.shape)\n",
        "    assert len(label) == len(psg)\n",
        "\n",
        "    # in ISRUC, 0-Wake, 1-N1, 2-N2, 3-N3, 5-REM\n",
        "    label[label==5] = 4  # make 4 correspond to REM\n",
        "    fold_label.append(np.eye(5)[label])\n",
        "    fold_psg.append(psg)\n",
        "    fold_len.append(len(label))\n",
        "print('Preprocess over.')\n",
        "\n",
        "np.savez(path.join(path_output, 'ISRUC_S3.npz'),\n",
        "    Fold_data = fold_psg,\n",
        "    Fold_label = fold_label,\n",
        "    Fold_len = fold_len\n",
        ")\n",
        "print('Saved to', path.join(path_output, 'ISRUC_S3.npz'))\n"
      ],
      "metadata": {
        "id": "3b_2DE4CDmsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class kFoldGenerator():\n",
        "    '''\n",
        "    Data Generator\n",
        "    '''\n",
        "    k = -1      # the fold number\n",
        "    x_list = [] # x list with length=k\n",
        "    y_list = [] # x list with length=k\n",
        "\n",
        "    # Initializate\n",
        "    def __init__(self, x, y):\n",
        "        if len(x) != len(y):\n",
        "            assert False, 'Data generator: Length of x or y is not equal to k.'\n",
        "        self.k = len(x)\n",
        "        self.x_list = x\n",
        "        self.y_list = y\n",
        "\n",
        "    # Get i-th fold\n",
        "    def getFold(self, i):\n",
        "        isFirst = True\n",
        "        for p in range(self.k):\n",
        "            if p != i:\n",
        "                if isFirst:\n",
        "                    train_data = self.x_list[p]\n",
        "                    train_targets = self.y_list[p]\n",
        "                    isFirst = False\n",
        "                else:\n",
        "                    train_data = np.concatenate((train_data, self.x_list[p]))\n",
        "                    train_targets = np.concatenate((train_targets, self.y_list[p]))\n",
        "            else:\n",
        "                val_data = self.x_list[p]\n",
        "                val_targets = self.y_list[p]\n",
        "        return train_data, train_targets, val_data, val_targets\n",
        "\n",
        "    # Get all data x\n",
        "    def getX(self):\n",
        "        All_X = self.x_list[0]\n",
        "        for i in range(1, self.k):\n",
        "            All_X = np.append(All_X, self.x_list[i], axis=0)\n",
        "        return All_X\n",
        "\n",
        "    # Get all label y (one-hot)\n",
        "    def getY(self):\n",
        "        All_Y = self.y_list[0][2:-2]\n",
        "        for i in range(1, self.k):\n",
        "            All_Y = np.append(All_Y, self.y_list[i][2:-2], axis=0)\n",
        "        return All_Y\n",
        "\n",
        "    # Get all label y (int)\n",
        "    def getY_int(self):\n",
        "        All_Y = self.getY()\n",
        "        return np.argmax(All_Y, axis=1)\n",
        "\n",
        "\n",
        "class DominGenerator():\n",
        "    '''\n",
        "    Domin Generator\n",
        "    '''\n",
        "    k = -1       # the fold number\n",
        "    l_list = []  # length of each domin\n",
        "    d_list = []  # d list with length=k\n",
        "\n",
        "    # Initializate\n",
        "    def __init__(self, len_list):\n",
        "        self.l_list = len_list\n",
        "        self.k = len(len_list)\n",
        "\n",
        "    # Get i-th fold\n",
        "    def getFold(self, i):\n",
        "        isFirst = True\n",
        "        isFirstVal = True\n",
        "        j = 0   #1~9\n",
        "        ii = 0  #1~10\n",
        "        for l in self.l_list:\n",
        "            if ii != i:\n",
        "                a = np.zeros((l, 9), dtype=int)\n",
        "                a[:, j] = 1\n",
        "                if isFirst:\n",
        "                    train_domin = a\n",
        "                    isFirst = False\n",
        "                else:\n",
        "                    train_domin = np.concatenate((train_domin, a))\n",
        "                j += 1\n",
        "            else:\n",
        "                if isFirstVal:\n",
        "                    val_domin = np.zeros((l, 9), dtype=int)\n",
        "                    isFirstVal = False\n",
        "                else:\n",
        "                    a = np.zeros((l, 9), dtype=int)\n",
        "                    val_domin = np.concatenate((val_domin, a))\n",
        "            ii += 1\n",
        "        return train_domin, val_domin\n"
      ],
      "metadata": {
        "id": "hjFwFqRkFx8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv1D, Dense, Dropout, MaxPool1D, Activation\n",
        "from keras.layers import Flatten, Reshape, TimeDistributed, BatchNormalization\n",
        "\n",
        "'''\n",
        "A Feature Extractor Network\n",
        "'''\n",
        "\n",
        "def build_FeatureNet(opt, channels=10, time_second=30, freq=100):\n",
        "    activation = tf.nn.relu\n",
        "    padding = 'same'\n",
        "\n",
        "    ######### Input ########\n",
        "    input_signal = Input(shape=(time_second * freq, 1), name='input_signal')\n",
        "\n",
        "    ######### CNNs with small filter size at the first layer #########\n",
        "    cnn0 = Conv1D(kernel_size=50,\n",
        "                  filters=32,\n",
        "                  strides=6,\n",
        "                  kernel_regularizer=keras.regularizers.l2(0.001))\n",
        "    s = cnn0(input_signal)\n",
        "    s = BatchNormalization()(s)\n",
        "    s = Activation(activation=activation)(s)\n",
        "    cnn1 = MaxPool1D(pool_size=16, strides=16)\n",
        "    s = cnn1(s)\n",
        "    cnn2 = Dropout(0.5)\n",
        "    s = cnn2(s)\n",
        "    cnn3 = Conv1D(kernel_size=8, filters=64, strides=1, padding=padding)\n",
        "    s = cnn3(s)\n",
        "    s = BatchNormalization()(s)\n",
        "    s = Activation(activation=activation)(s)\n",
        "    cnn4 = Conv1D(kernel_size=8, filters=64, strides=1, padding=padding)\n",
        "    s = cnn4(s)\n",
        "    s = BatchNormalization()(s)\n",
        "    s = Activation(activation=activation)(s)\n",
        "    cnn5 = Conv1D(kernel_size=8, filters=64, strides=1, padding=padding)\n",
        "    s = cnn5(s)\n",
        "    s = BatchNormalization()(s)\n",
        "    s = Activation(activation=activation)(s)\n",
        "    cnn6 = MaxPool1D(pool_size=8, strides=8)\n",
        "    s = cnn6(s)\n",
        "    cnn7 = Reshape((int(s.shape[1]) * int(s.shape[2]), ))  # Flatten\n",
        "    s = cnn7(s)\n",
        "\n",
        "    ######### CNNs with large filter size at the first layer #########\n",
        "    cnn8 = Conv1D(kernel_size=400,\n",
        "                  filters=64,\n",
        "                  strides=50,\n",
        "                  kernel_regularizer=keras.regularizers.l2(0.001))\n",
        "    l = cnn8(input_signal)\n",
        "    l = BatchNormalization()(l)\n",
        "    l = Activation(activation=activation)(l)\n",
        "    cnn9 = MaxPool1D(pool_size=8, strides=8)\n",
        "    l = cnn9(l)\n",
        "    cnn10 = Dropout(0.5)\n",
        "    l = cnn10(l)\n",
        "    cnn11 = Conv1D(kernel_size=6, filters=64, strides=1, padding=padding)\n",
        "    l = cnn11(l)\n",
        "    l = BatchNormalization()(l)\n",
        "    l = Activation(activation=activation)(l)\n",
        "    cnn12 = Conv1D(kernel_size=6, filters=64, strides=1, padding=padding)\n",
        "    l = cnn12(l)\n",
        "    l = BatchNormalization()(l)\n",
        "    l = Activation(activation=activation)(l)\n",
        "    cnn13 = Conv1D(kernel_size=6, filters=64, strides=1, padding=padding)\n",
        "    l = cnn13(l)\n",
        "    l = BatchNormalization()(l)\n",
        "    l = Activation(activation=activation)(l)\n",
        "    cnn14 = MaxPool1D(pool_size=4, strides=4)\n",
        "    l = cnn14(l)\n",
        "    cnn15 = Reshape((int(l.shape[1]) * int(l.shape[2]), ))\n",
        "    l = cnn15(l)\n",
        "\n",
        "    feature = keras.layers.concatenate([s, l])\n",
        "\n",
        "    fea_part = Model(input_signal, feature)\n",
        "\n",
        "    ##################################################\n",
        "\n",
        "    input = Input(shape=(channels, time_second * freq), name='input_signal')\n",
        "    reshape = Reshape((channels, time_second * freq, 1))  # Flatten\n",
        "    input_re = reshape(input)\n",
        "    fea_all = TimeDistributed(fea_part)(input_re)\n",
        "\n",
        "    merged = Flatten()(fea_all)\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    merged = Dense(64)(merged)\n",
        "    merged = Dense(5)(merged)\n",
        "\n",
        "    fea_softmax = Activation(activation='softmax')(merged)\n",
        "\n",
        "    # FeatureNet with softmax\n",
        "    fea_model = Model(input, fea_softmax)\n",
        "    fea_model.compile(optimizer=opt,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['acc'])\n",
        "\n",
        "    # FeatureNet without softmax\n",
        "    pre_model = Model(input, fea_all)\n",
        "    pre_model.compile(optimizer=opt,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['acc'])\n",
        "\n",
        "    return fea_model, pre_model\n"
      ],
      "metadata": {
        "id": "BLwrQK63F9OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import configparser\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from scipy.sparse.linalg import eigs\n",
        "import keras\n",
        "\n",
        "\n",
        "##########################################################################################\n",
        "# Read configuration file ################################################################\n",
        "\n",
        "def ReadConfig(configfile):\n",
        "    config = configparser.ConfigParser()\n",
        "    print('Config: ', configfile)\n",
        "    config.read(configfile)\n",
        "    cfgPath = config['path']\n",
        "    cfgFeat = config['feature']\n",
        "    cfgTrain = config['train']\n",
        "    cfgModel = config['model']\n",
        "    return cfgPath, cfgFeat, cfgTrain, cfgModel\n",
        "\n",
        "##########################################################################################\n",
        "# Add context to the origin data and label ###############################################\n",
        "\n",
        "def AddContext_MultiSub(x, y, Fold_Num, context, i):\n",
        "    '''\n",
        "    input:\n",
        "        x       : [N,V,F];\n",
        "        y       : [N,C]; (C:num_of_classes)\n",
        "        Fold_Num: [kfold];\n",
        "        context : int;\n",
        "        i       : int (i-th fold)\n",
        "    return:\n",
        "        x with contexts. [N',V,F]\n",
        "    '''\n",
        "    cut = context // 2\n",
        "    fold = Fold_Num.copy()\n",
        "    fold = np.delete(fold, -1)\n",
        "    id_del = np.concatenate([np.cumsum(fold) - i for i in range(1, context)])\n",
        "    id_del = np.sort(id_del)\n",
        "\n",
        "    x_c = np.zeros([x.shape[0] - 2 * cut, context, x.shape[1], x.shape[2]], dtype=float)\n",
        "    for j in range(cut, x.shape[0] - cut):\n",
        "        x_c[j - cut] = x[j - cut:j + cut + 1]\n",
        "\n",
        "    x_c = np.delete(x_c, id_del, axis=0)\n",
        "    y_c = np.delete(y[cut: -cut], id_del, axis=0)\n",
        "    return x_c, y_c\n",
        "\n",
        "def AddContext_SingleSub(x, y, context):\n",
        "    cut = int(context / 2)\n",
        "    x_c = np.zeros([x.shape[0] - 2 * cut, context, x.shape[1], x.shape[2]], dtype=float)\n",
        "    for i in range(cut, x.shape[0] - cut):\n",
        "        x_c[i - cut] = x[i - cut:i + cut + 1]\n",
        "    y_c = y[cut:-cut]\n",
        "    return x_c, y_c\n",
        "\n",
        "##########################################################################################\n",
        "# Instantiation operation ################################################################\n",
        "\n",
        "def Instantiation_optim(name, lr):\n",
        "    if   name==\"adam\":\n",
        "        opt = keras.optimizers.Adam(lr=lr)\n",
        "    elif name==\"RMSprop\":\n",
        "        opt = keras.optimizers.RMSprop(lr=lr)\n",
        "    elif name==\"SGD\":\n",
        "        opt = keras.optimizers.SGD(lr=lr)\n",
        "    else:\n",
        "        assert False,'Config: check optimizer, may be not implemented.'\n",
        "    return opt\n",
        "\n",
        "def Instantiation_regularizer(l1, l2):\n",
        "    if   l1!=0 and l2!=0:\n",
        "        regularizer = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
        "    elif l1!=0 and l2==0:\n",
        "        regularizer = keras.regularizers.l1(l1)\n",
        "    elif l1==0 and l2!=0:\n",
        "        regularizer = keras.regularizers.l2(l2)\n",
        "    else:\n",
        "        regularizer = None\n",
        "    return regularizer\n",
        "\n",
        "##########################################################################################\n",
        "# Print score between Ytrue and Ypred ####################################################\n",
        "\n",
        "def PrintScore(true, pred, savePath=None, average='macro'):\n",
        "    # savePath=None -> console, else to Result.txt\n",
        "    if savePath == None:\n",
        "        saveFile = None\n",
        "    else:\n",
        "        saveFile = open(savePath + \"Result.txt\", 'a+')\n",
        "    # Main scores\n",
        "    F1 = metrics.f1_score(true, pred, average=None)\n",
        "    print(\"Main scores:\")\n",
        "    print('Acc\\tF1S\\tKappa\\tF1_W\\tF1_N1\\tF1_N2\\tF1_N3\\tF1_R', file=saveFile)\n",
        "    print('%.4f\\t%.4f\\t%.4f\\t%.4f\\t%.4f\\t%.4f\\t%.4f\\t%.4f' %\n",
        "          (metrics.accuracy_score(true, pred),\n",
        "           metrics.f1_score(true, pred, average=average),\n",
        "           metrics.cohen_kappa_score(true, pred),\n",
        "           F1[0], F1[1], F1[2], F1[3], F1[4]),\n",
        "          file=saveFile)\n",
        "    # Classification report\n",
        "    print(\"\\nClassification report:\", file=saveFile)\n",
        "    print(metrics.classification_report(true, pred,\n",
        "                                        target_names=['Wake','N1','N2','N3','REM'],\n",
        "                                        digits=4), file=saveFile)\n",
        "    # Confusion matrix\n",
        "    print('Confusion matrix:', file=saveFile)\n",
        "    print(metrics.confusion_matrix(true,pred), file=saveFile)\n",
        "    # Overall scores\n",
        "    print('\\n    Accuracy\\t',metrics.accuracy_score(true,pred), file=saveFile)\n",
        "    print(' Cohen Kappa\\t',metrics.cohen_kappa_score(true,pred), file=saveFile)\n",
        "    print('    F1-Score\\t',metrics.f1_score(true,pred,average=average), '\\tAverage =',average, file=saveFile)\n",
        "    print('   Precision\\t',metrics.precision_score(true,pred,average=average), '\\tAverage =',average, file=saveFile)\n",
        "    print('      Recall\\t',metrics.recall_score(true,pred,average=average), '\\tAverage =',average, file=saveFile)\n",
        "    if savePath != None:\n",
        "        saveFile.close()\n",
        "    return\n",
        "\n",
        "##########################################################################################\n",
        "# Print confusion matrix and save ########################################################\n",
        "\n",
        "def ConfusionMatrix(y_true, y_pred, classes, savePath, title=None, cmap=plt.cm.Blues):\n",
        "    if not title:\n",
        "        title = 'Confusion matrix'\n",
        "    # Compute confusion matrix\n",
        "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
        "    cm_n=cm\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    fig, ax = plt.subplots(figsize=(5, 4))\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation_mode=\"anchor\")\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j]*100,'.2f')+'%\\n'+format(cm_n[i, j],'d'),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(savePath+title+\".png\")\n",
        "    plt.show()\n",
        "    return ax\n",
        "\n",
        "##########################################################################################\n",
        "# Draw ACC / loss curve and save #########################################################\n",
        "\n",
        "def VariationCurve(fit,val,yLabel,savePath,figsize=(9, 6)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.plot(range(1,len(fit)+1), fit,label='Train')\n",
        "    plt.plot(range(1,len(val)+1), val, label='Val')\n",
        "    plt.title('Model ' + yLabel)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(yLabel)\n",
        "    plt.legend()\n",
        "    plt.savefig(savePath + 'Model_' + yLabel + '.png')\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "# compute \\tilde{L}\n",
        "\n",
        "def scaled_Laplacian(W):\n",
        "    '''\n",
        "    compute \\tilde{L}\n",
        "    ----------\n",
        "    Parameters\n",
        "    W: np.ndarray, shape is (N, N), N is the num of vertices\n",
        "    ----------\n",
        "    Returns\n",
        "    scaled_Laplacian: np.ndarray, shape (N, N)\n",
        "    '''\n",
        "    assert W.shape[0] == W.shape[1]\n",
        "    D = np.diag(np.sum(W, axis = 1))\n",
        "    L = D - W\n",
        "    lambda_max = eigs(L, k = 1, which = 'LR')[0].real\n",
        "    return (2 * L) / lambda_max - np.identity(W.shape[0])\n",
        "\n",
        "##########################################################################################\n",
        "# compute a list of chebyshev polynomials from T_0 to T_{K-1} ############################\n",
        "\n",
        "def cheb_polynomial(L_tilde, K):\n",
        "    '''\n",
        "    compute a list of chebyshev polynomials from T_0 to T_{K-1}\n",
        "    ----------\n",
        "    Parameters\n",
        "    L_tilde: scaled Laplacian, np.ndarray, shape (N, N)\n",
        "    K: the maximum order of chebyshev polynomials\n",
        "    ----------\n",
        "    Returns\n",
        "    cheb_polynomials: list(np.ndarray), length: K, from T_0 to T_{K-1}\n",
        "    '''\n",
        "    N = L_tilde.shape[0]\n",
        "    cheb_polynomials = np.array([np.identity(N), L_tilde.copy()])\n",
        "    for i in range(2, K):\n",
        "        cheb_polynomials = np.append(\n",
        "            cheb_polynomials,\n",
        "            [2 * L_tilde * cheb_polynomials[i - 1] - cheb_polynomials[i - 2]],\n",
        "            axis=0)\n",
        "    return cheb_polynomials\n"
      ],
      "metadata": {
        "id": "uAVz58TwGBwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import shutil\n",
        "import gc\n",
        "import os\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "#import keras.backend.tensorflow_backend as KTF\n",
        "from tensorflow.keras import backend as KTF\n",
        "# from model.DataGenerator import kFoldGenerator\n",
        "# from model.FeatureNet import build_FeatureNet\n",
        "# from model.Utils import *"
      ],
      "metadata": {
        "id": "WaKo9gxfEv4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(128 * '#')\n",
        "print('Start to train FeatureNet.')\n",
        "\n",
        "# # 1. Get configuration\n",
        "\n",
        "# ## 1.1. Read .config file\n",
        "\n",
        "# command line parameters -c -g\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"-c\", type = str, help = \"configuration file\", required = True)\n",
        "parser.add_argument(\"-g\", type = str, help = \"GPU number to use, set '-1' to use CPU\", required = True)\n",
        "args = parser.parse_args()\n",
        "Path, cfgFeature, _, _ = ReadConfig(args.c)\n",
        "\n",
        "# set GPU number or use CPU only\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.g\n",
        "if args.g != \"-1\":\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth=True\n",
        "    sess = tf.Session(config=config)\n",
        "    KTF.set_session(sess)\n",
        "    print(\"Use GPU #\"+args.g)\n",
        "else:\n",
        "    print(\"Use CPU only\")\n",
        "\n",
        "# ## 1.2. Analytic parameters\n",
        "\n",
        "# [train] parameters ('_f' means FeatureNet)\n",
        "channels   = int(cfgFeature[\"channels\"])\n",
        "fold       = int(cfgFeature[\"fold\"])\n",
        "num_epochs_f = int(cfgFeature[\"epoch_f\"])\n",
        "batch_size_f = int(cfgFeature[\"batch_size_f\"])\n",
        "optimizer_f  = cfgFeature[\"optimizer_f\"]\n",
        "learn_rate_f = float(cfgFeature[\"learn_rate_f\"])\n",
        "\n",
        "\n",
        "# ## 1.3. Parameter check and enable\n",
        "\n",
        "# Create save pathand copy .config to it\n",
        "if not os.path.exists(Path['Save']):\n",
        "    os.makedirs(Path['Save'])\n",
        "shutil.copyfile(args.c, Path['Save']+\"last.config\")\n",
        "\n",
        "\n",
        "# # 2. Read data and process data\n",
        "\n",
        "# ## 2.1. Read data\n",
        "# Each fold corresponds to one subject's data (ISRUC-S3 dataset)\n",
        "ReadList = np.load(Path['data'], allow_pickle=True)\n",
        "Fold_Num   = ReadList['Fold_len']    # Num of samples of each fold\n",
        "Fold_Data  = ReadList['Fold_data']   # Data of each fold\n",
        "Fold_Label = ReadList['Fold_label']  # Labels of each fold\n",
        "\n",
        "print(\"Read data successfully\")\n",
        "print('Number of samples: ',np.sum(Fold_Num))\n",
        "\n",
        "# ## 2.2. Build kFoldGenerator or DominGenerator\n",
        "DataGenerator = kFoldGenerator(Fold_Data, Fold_Label)\n",
        "\n",
        "\n",
        "# # 3. Model training (cross validation)\n",
        "\n",
        "# k-fold cross validation\n",
        "all_scores = []\n",
        "for i in range(fold):\n",
        "    print(128*'_')\n",
        "    print('Fold #', i)\n",
        "\n",
        "    # Instantiation optimizer\n",
        "    opt_f = Instantiation_optim(optimizer_f, learn_rate_f) # optimizer of FeatureNet\n",
        "\n",
        "    # get i th-fold data\n",
        "    train_data,train_targets,val_data,val_targets = DataGenerator.getFold(i)\n",
        "\n",
        "    ## build FeatureNet & train\n",
        "    featureNet, featureNet_p = build_FeatureNet(opt_f, channels) # '_p' model is without the softmax layer\n",
        "    history_fea = featureNet.fit(\n",
        "        x = train_data,\n",
        "        y = train_targets,\n",
        "        epochs = num_epochs_f,\n",
        "        batch_size = batch_size_f,\n",
        "        shuffle = True,\n",
        "        validation_data = (val_data, val_targets),\n",
        "        verbose = 2,\n",
        "        callbacks=[keras.callbacks.ModelCheckpoint(Path['Save']+'FeatureNet_Best_'+str(i)+'.h5',\n",
        "                                                   monitor='val_acc',\n",
        "                                                   verbose=0,\n",
        "                                                   save_best_only=True,\n",
        "                                                   save_weights_only=False,\n",
        "                                                   mode='auto',\n",
        "                                                   period=1 )])\n",
        "    # Save training information\n",
        "    if i==0:\n",
        "        fit_loss = np.array(history_fea.history['loss'])*Fold_Num[i]\n",
        "        fit_acc = np.array(history_fea.history['acc'])*Fold_Num[i]\n",
        "        fit_val_loss = np.array(history_fea.history['val_loss'])*Fold_Num[i]\n",
        "        fit_val_acc = np.array(history_fea.history['val_acc'])*Fold_Num[i]\n",
        "    else:\n",
        "        fit_loss = fit_loss+np.array(history_fea.history['loss'])*Fold_Num[i]\n",
        "        fit_acc = fit_acc+np.array(history_fea.history['acc'])*Fold_Num[i]\n",
        "        fit_val_loss = fit_val_loss+np.array(history_fea.history['val_loss'])*Fold_Num[i]\n",
        "        fit_val_acc = fit_val_acc+np.array(history_fea.history['val_acc'])*Fold_Num[i]\n",
        "\n",
        "    # load the weights of best performance\n",
        "    featureNet.load_weights(Path['Save']+'FeatureNet_Best_'+str(i)+'.h5')\n",
        "\n",
        "    # get and save the learned feature\n",
        "    train_feature = featureNet_p.predict(train_data)\n",
        "    val_feature = featureNet_p.predict(val_data)\n",
        "    print('Save feature of Fold #' + str(i) + ' to' + Path['Save']+'Feature_'+str(i) + '.npz')\n",
        "    np.savez(Path['Save']+'Feature_'+str(i)+'.npz',\n",
        "        train_feature = train_feature,\n",
        "        val_feature = val_feature,\n",
        "        train_targets = train_targets,\n",
        "        val_targets = val_targets\n",
        "    )\n",
        "\n",
        "    saveFile = open(Path['Save'] + \"Result_FeatureNet.txt\", 'a+')\n",
        "    print('Fold #'+str(i), file=saveFile)\n",
        "    print(history_fea.history, file=saveFile)\n",
        "    saveFile.close()\n",
        "\n",
        "    # Fold finish\n",
        "    keras.backend.clear_session()\n",
        "    del featureNet, featureNet_p, train_data, train_targets, val_data, val_targets, train_feature, val_feature\n",
        "    gc.collect()\n",
        "\n",
        "print(128 * '_')\n",
        "\n",
        "print('End of training FeatureNet.')\n",
        "print(128 * '#')\n"
      ],
      "metadata": {
        "id": "J-sbSQy7EkGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tf-nightly\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.layers import Layer\n",
        "from keras.layers.core import Dropout, Lambda\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.contrib import layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PlG2ZnJ4HXvm",
        "outputId": "6a8937d0-ba62-4047-ec2b-cbcc689db822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf-nightly\n",
            "  Downloading tf_nightly-2.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (590.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.0/590.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.2.0)\n",
            "Collecting h5py>=3.10.0 (from tf-nightly)\n",
            "  Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (16.0.6)\n",
            "Collecting ml-dtypes~=0.3.1 (from tf-nightly)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (4.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.62.1)\n",
            "Collecting tb-nightly~=2.16.0.a (from tf-nightly)\n",
            "  Downloading tb_nightly-2.16.0a20240212-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-nightly~=3.0.0.dev (from tf-nightly)\n",
            "  Downloading keras_nightly-3.0.4.dev2024021403-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tf-nightly) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-nightly~=3.0.0.dev->tf-nightly) (13.7.1)\n",
            "Collecting namex (from keras-nightly~=3.0.0.dev->tf-nightly)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-nightly~=3.0.0.dev->tf-nightly) (0.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.16.0.a->tf-nightly) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.16.0.a->tf-nightly) (0.7.2)\n",
            "Collecting tf-keras-nightly (from tb-nightly~=2.16.0.a->tf-nightly)\n",
            "  Downloading tf_keras_nightly-2.17.0.dev2024031809-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.16.0.a->tf-nightly) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly~=2.16.0.a->tf-nightly) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nightly~=3.0.0.dev->tf-nightly) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nightly~=3.0.0.dev->tf-nightly) (2.16.1)\n",
            "Collecting tf-nightly\n",
            "  Downloading tf_nightly-2.17.0.dev20240318-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (579.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m579.4/579.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tb-nightly~=2.17.0.a (from tf-nightly)\n",
            "  Downloading tb_nightly-2.17.0a20240318-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-nightly~=3.1.0.dev (from tf-nightly)\n",
            "  Downloading keras_nightly-3.1.0.dev2024031803-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optree (from keras-nightly~=3.1.0.dev->tf-nightly)\n",
            "  Downloading optree-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.8/286.8 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nightly~=3.0.0.dev->tf-nightly) (0.1.2)\n",
            "Installing collected packages: namex, optree, ml-dtypes, h5py, tb-nightly, keras-nightly, tf-nightly\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h5py-3.10.0 keras-nightly-3.1.0.dev2024031803 ml-dtypes-0.3.2 namex-0.0.7 optree-0.10.0 tb-nightly-2.17.0a20240318 tf-nightly-2.17.0.dev20240318\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py",
                  "keras",
                  "ml_dtypes",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "23628894f7a1451fa92c6b098e9f53be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.layers.core'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-81fe80fda924>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.core'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Model code of MSTGCN.\n",
        "--------\n",
        "Model input:  (*, T, V, F)\n",
        "    T: num_of_timesteps\n",
        "    V: num_of_vertices\n",
        "    F: num_of_features\n",
        "Model output: (*, 5)\n",
        "'''\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "################################################################################################\n",
        "# Attention Layers\n",
        "\n",
        "class TemporalAttention(Layer):\n",
        "    '''\n",
        "    compute temporal attention scores\n",
        "    --------\n",
        "    Input:  (batch_size, num_of_timesteps, num_of_vertices, num_of_features)\n",
        "    Output: (batch_size, num_of_timesteps, num_of_timesteps)\n",
        "    '''\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TemporalAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_of_timesteps, num_of_vertices, num_of_features = input_shape\n",
        "        self.U_1 = self.add_weight(name='U_1',\n",
        "                                   shape=(num_of_vertices, 1),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_2 = self.add_weight(name='U_2',\n",
        "                                   shape=(num_of_features, num_of_vertices),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_3 = self.add_weight(name='U_3',\n",
        "                                   shape=(num_of_features, ),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.b_e = self.add_weight(name='b_e',\n",
        "                                   shape=(1, num_of_timesteps, num_of_timesteps),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_e = self.add_weight(name='V_e',\n",
        "                                   shape=(num_of_timesteps, num_of_timesteps),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        super(TemporalAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        _, T, V, F = x.shape\n",
        "\n",
        "        # shape of lhs is (batch_size, V, T)\n",
        "        lhs = K.dot(tf.transpose(x, perm=[0,1,3,2]), self.U_1)\n",
        "        lhs = tf.reshape(lhs, [tf.shape(x)[0], T, F])\n",
        "        lhs = K.dot(lhs, self.U_2)\n",
        "\n",
        "        # shape of rhs is (batch_size, T, V)\n",
        "        rhs = K.dot(self.U_3, tf.transpose(x,perm=[2,0,3,1]))\n",
        "        rhs = tf.transpose(rhs, perm=[1,0,2])\n",
        "\n",
        "        # shape of product is (batch_size, V, V)\n",
        "        product = K.batch_dot(lhs, rhs)\n",
        "\n",
        "        S = tf.transpose(K.dot(self.V_e, tf.transpose(K.sigmoid(product + self.b_e),perm=[1, 2, 0])),perm=[2, 0, 1])\n",
        "\n",
        "        # normalization\n",
        "        S = S - K.max(S, axis = 1, keepdims = True)\n",
        "        exp = K.exp(S)\n",
        "        S_normalized = exp / K.sum(exp, axis = 1, keepdims = True)\n",
        "        return S_normalized\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], input_shape[1])\n",
        "\n",
        "\n",
        "class SpatialAttention(Layer):\n",
        "    '''\n",
        "    compute spatial attention scores\n",
        "    --------\n",
        "    Input:  (batch_size, num_of_timesteps, num_of_vertices, num_of_features)\n",
        "    Output: (batch_size, num_of_vertices, num_of_vertices)\n",
        "    '''\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SpatialAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_of_timesteps, num_of_vertices, num_of_features = input_shape\n",
        "        self.W_1 = self.add_weight(name='W_1',\n",
        "                                   shape=(num_of_timesteps, 1),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.W_2 = self.add_weight(name='W_2',\n",
        "                                   shape=(num_of_features, num_of_timesteps),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.W_3 = self.add_weight(name='W_3',\n",
        "                                   shape=(num_of_features, ),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.b_s = self.add_weight(name='b_s',\n",
        "                                   shape=(1, num_of_vertices, num_of_vertices),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_s = self.add_weight(name='V_s',\n",
        "                                   shape=(num_of_vertices, num_of_vertices),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        super(SpatialAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        _, T, V, F = x.shape\n",
        "\n",
        "        # shape of lhs is (batch_size, V, T)\n",
        "        lhs = K.dot(tf.transpose(x, perm=[0,2,3,1]), self.W_1)\n",
        "        lhs = tf.reshape(lhs,[tf.shape(x)[0], V, F])\n",
        "        lhs = K.dot(lhs, self.W_2)\n",
        "\n",
        "        # shape of rhs is (batch_size, T, V)\n",
        "        rhs = K.dot(self.W_3, tf.transpose(x, perm=[1,0,3,2]))\n",
        "        rhs = tf.transpose(rhs, perm=[1,0,2])\n",
        "\n",
        "        # shape of product is (batch_size, V, V)\n",
        "        product = K.batch_dot(lhs, rhs)\n",
        "\n",
        "        S = tf.transpose(K.dot(self.V_s, tf.transpose(K.sigmoid(product + self.b_s),perm=[1, 2, 0])),perm=[2, 0, 1])\n",
        "\n",
        "        # normalization\n",
        "        S = S - K.max(S, axis = 1, keepdims = True)\n",
        "        exp = K.exp(S)\n",
        "        S_normalized = exp / K.sum(exp, axis = 1, keepdims = True)\n",
        "        return S_normalized\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[2], input_shape[2])\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "################################################################################################\n",
        "# Adaptive Graph Learning Layer\n",
        "\n",
        "def diff_loss(diff, S):\n",
        "    '''\n",
        "    compute the 1st loss of L_{graph_learning}\n",
        "    '''\n",
        "    if len(S.shape) == 4:\n",
        "        # batch input\n",
        "        return K.mean(K.sum(K.sum(diff**2, axis=3) * S, axis=(1, 2)))\n",
        "    else:\n",
        "        return K.sum(K.sum(diff**2, axis=2) * S)\n",
        "\n",
        "\n",
        "def F_norm_loss(S, Falpha):\n",
        "    '''\n",
        "    compute the 2nd loss of L_{graph_learning}\n",
        "    '''\n",
        "    if len(S.shape) == 4:\n",
        "        # batch input\n",
        "        return Falpha * K.sum(K.mean(S**2, axis=0))\n",
        "    else:\n",
        "        return Falpha * K.sum(S**2)\n",
        "\n",
        "\n",
        "class Graph_Learn(Layer):\n",
        "    '''\n",
        "    Graph structure learning (based on the middle time slice)\n",
        "    --------\n",
        "    Input:  (batch_size, num_of_timesteps, num_of_vertices, num_of_features)\n",
        "    Output: (batch_size, num_of_vertices, num_of_vertices)\n",
        "    '''\n",
        "    def __init__(self, alpha, **kwargs):\n",
        "        self.alpha = alpha\n",
        "        self.S = tf.convert_to_tensor([[[0.0]]])  # similar to placeholder\n",
        "        self.diff = tf.convert_to_tensor([[[[0.0]]]])  # similar to placeholder\n",
        "        super(Graph_Learn, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_of_timesteps, num_of_vertices, num_of_features = input_shape\n",
        "        self.a = self.add_weight(name='a',\n",
        "                                 shape=(num_of_features, 1),\n",
        "                                 initializer='uniform',\n",
        "                                 trainable=True)\n",
        "        # add loss L_{graph_learning} in the layer\n",
        "        self.add_loss(F_norm_loss(self.S, self.alpha))\n",
        "        self.add_loss(diff_loss(self.diff, self.S))\n",
        "        super(Graph_Learn, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        _, T, V, F = x.shape\n",
        "        N = tf.shape(x)[0]\n",
        "\n",
        "        outputs = []\n",
        "        diff_tmp = 0\n",
        "        for time_step in range(T):\n",
        "            # shape: (N,V,F) use the current slice\n",
        "            xt = x[:, time_step, :, :]\n",
        "            # shape: (N,V,V)\n",
        "            diff = tf.transpose(tf.broadcast_to(xt, [V,N,V,F]), perm=[2,1,0,3]) - xt\n",
        "            # shape: (N,V,V)\n",
        "            tmpS = K.exp(K.reshape(K.dot(tf.transpose(K.abs(diff), perm=[1,0,2,3]), self.a), [N,V,V]))\n",
        "            # normalization\n",
        "            S = tmpS / tf.transpose(tf.broadcast_to(K.sum(tmpS, axis=1), [V,N,V]), perm=[1,2,0])\n",
        "\n",
        "            diff_tmp += K.abs(diff)\n",
        "            outputs.append(S)\n",
        "\n",
        "        outputs = tf.transpose(outputs, perm=[1,0,2,3])\n",
        "        self.S = K.mean(outputs, axis=0)\n",
        "        self.diff = K.mean(diff_tmp, axis=0) /tf.convert_to_tensor(int(T), tf.float32)\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # shape: (n, num_of_vertices,num_of_vertices, num_of_vertices)\n",
        "        return (input_shape[0],input_shape[1],input_shape[2],input_shape[2])\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "################################################################################################\n",
        "# GCN layers\n",
        "\n",
        "class cheb_conv_with_Att_GL(Layer):\n",
        "    '''\n",
        "    K-order chebyshev graph convolution with attention after Graph Learn\n",
        "    --------\n",
        "    Input:  [x   (batch_size, num_of_timesteps, num_of_vertices, num_of_features),\n",
        "             Att (batch_size, num_of_vertices, num_of_vertices),\n",
        "             S   (batch_size, num_of_vertices, num_of_vertices)]\n",
        "    Output: (batch_size, num_of_timesteps, num_of_vertices, num_of_filters)\n",
        "    '''\n",
        "    def __init__(self, num_of_filters, k, **kwargs):\n",
        "        self.k = k\n",
        "        self.num_of_filters = num_of_filters\n",
        "        super(cheb_conv_with_Att_GL, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        x_shape, Att_shape, S_shape = input_shape\n",
        "        _, T, V, F = x_shape\n",
        "        self.Theta = self.add_weight(name='Theta',\n",
        "                                     shape=(self.k, F, self.num_of_filters),\n",
        "                                     initializer='uniform',\n",
        "                                     trainable=True)\n",
        "        super(cheb_conv_with_Att_GL, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        #Input:  [x, Att, S]\n",
        "        assert isinstance(x, list)\n",
        "        assert len(x)==3, 'Cheb_gcn input error'\n",
        "        x, Att, S = x\n",
        "        _, T, V, F = x.shape\n",
        "\n",
        "        S = K.minimum(S, tf.transpose(S,perm=[0,1,3,2])) # Ensure symmetry\n",
        "\n",
        "        # GCN\n",
        "        outputs=[]\n",
        "        for time_step in range(T):\n",
        "            # shape of x is (batch_size, V, F)\n",
        "            graph_signal = x[:, time_step, :, :]\n",
        "            output = K.zeros(shape=(tf.shape(x)[0], V, self.num_of_filters))\n",
        "\n",
        "            A = S[:, time_step, :, :]\n",
        "            #Calculating Chebyshev polynomials (let lambda_max=2)\n",
        "            D = tf.matrix_diag(K.sum(A, axis=1))\n",
        "            L = D - A\n",
        "            L_t = L - [tf.eye(int(V))]\n",
        "            cheb_polynomials = [tf.eye(int(V)), L_t]\n",
        "            for i in range(2, self.k):\n",
        "                cheb_polynomials.append(2 * L_t * cheb_polynomials[i - 1] - cheb_polynomials[i - 2])\n",
        "\n",
        "            for kk in range(self.k):\n",
        "                T_k = cheb_polynomials[kk]              # shape of T_k is (V, V)\n",
        "                T_k_with_at = T_k * Att                 # shape of T_k_with_at is (batch_size, V, V)\n",
        "                theta_k = self.Theta[kk]                # shape of theta_k is (F, num_of_filters)\n",
        "\n",
        "                # shape is (batch_size, V, F)\n",
        "                rhs = K.batch_dot(tf.transpose(T_k_with_at, perm=[0, 2, 1]), graph_signal)\n",
        "                output = output + K.dot(rhs, theta_k)\n",
        "            outputs.append(tf.expand_dims(output,-1))\n",
        "\n",
        "        return tf.transpose(K.relu(K.concatenate(outputs, axis=-1)), perm=[0,3,1,2])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # shape: (n, num_of_timesteps, num_of_vertices, num_of_filters)\n",
        "        return (input_shape[0][0], input_shape[0][1], input_shape[0][2], self.num_of_filters)\n",
        "\n",
        "\n",
        "class cheb_conv_with_Att_static(Layer):\n",
        "    '''\n",
        "    K-order chebyshev graph convolution with static graph structure\n",
        "    --------\n",
        "    Input:  [x   (batch_size, num_of_timesteps, num_of_vertices, num_of_features),\n",
        "             Att (batch_size, num_of_vertices, num_of_vertices)]\n",
        "    Output: (batch_size, num_of_timesteps, num_of_vertices, num_of_filters)\n",
        "    '''\n",
        "    def __init__(self, num_of_filters, k, cheb_polynomials, **kwargs):\n",
        "        self.k = k\n",
        "        self.num_of_filters = num_of_filters\n",
        "        self.cheb_polynomials = tf.to_float(cheb_polynomials)\n",
        "        super(cheb_conv_with_Att_static, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        x_shape, Att_shape = input_shape\n",
        "        _, T, V, F = x_shape\n",
        "        self.Theta = self.add_weight(name='Theta',\n",
        "                                     shape=(self.k, F, self.num_of_filters),\n",
        "                                     initializer='uniform',\n",
        "                                     trainable=True)\n",
        "        super(cheb_conv_with_Att_static, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        #Input:  [x, Att]\n",
        "        assert isinstance(x, list)\n",
        "        assert len(x) == 2, 'cheb_gcn error'\n",
        "        x, Att = x\n",
        "        _, T, V, F = x.shape\n",
        "\n",
        "        outputs = []\n",
        "        for time_step in range(T):\n",
        "            # shape is (batch_size, V, F)\n",
        "            graph_signal = x[:, time_step, :, :]\n",
        "            output = K.zeros(shape=(tf.shape(x)[0], V, self.num_of_filters))\n",
        "\n",
        "            for kk in range(self.k):\n",
        "                T_k = self.cheb_polynomials[kk]          # shape of T_k is (V, V)\n",
        "                T_k_with_at = K.dropout(T_k * Att, 0.6)  # shape of T_k_with_at is (batch_size, V, V)\n",
        "                theta_k = self.Theta[kk]                 # shape of theta_k is (F, num_of_filters)\n",
        "\n",
        "                # shape is (batch_size, V, F)\n",
        "                rhs = K.batch_dot(tf.transpose(T_k_with_at, perm=[0, 2, 1]), graph_signal)\n",
        "                output = output + K.dot(rhs, theta_k)\n",
        "            outputs.append(tf.expand_dims(output, -1))\n",
        "\n",
        "        return tf.transpose(K.relu(K.concatenate(outputs, axis=-1)), perm=[0, 3, 1, 2])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # shape: (n, num_of_timesteps, num_of_vertices, num_of_filters)\n",
        "        return (input_shape[0][0], input_shape[0][1], input_shape[0][2], self.num_of_filters)\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "################################################################################################\n",
        "# Some operations\n",
        "\n",
        "def reshape_dot(x):\n",
        "    #Input:  [x,TAtt]\n",
        "    x, TAtt = x\n",
        "    return tf.reshape(\n",
        "        K.batch_dot(\n",
        "            tf.reshape(tf.transpose(x, perm=[0, 2, 3, 1]),\n",
        "                       (tf.shape(x)[0], -1, tf.shape(x)[1])), TAtt),\n",
        "        [-1, x.shape[1], x.shape[2], x.shape[3]]\n",
        "    )\n",
        "\n",
        "\n",
        "def LayerNorm(x):\n",
        "    # do the layer normalization\n",
        "    relu_x = K.relu(x)\n",
        "    ln = tf.contrib.layers.layer_norm(relu_x, begin_norm_axis=3)\n",
        "    return ln\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "################################################################################################\n",
        "# Gradient Reverse Layer\n",
        "\n",
        "def reverse_gradient(X, hp_lambda):\n",
        "    \"\"\"Flips the sign of the incoming gradient during training.\"\"\"\n",
        "    num_calls=1\n",
        "    try:\n",
        "        reverse_gradient.num_calls =reverse_gradient.num_calls+ 1\n",
        "    except AttributeError:\n",
        "        reverse_gradient.num_calls = num_calls\n",
        "        num_calls=num_calls+1\n",
        "\n",
        "    grad_name = \"GradientReversal_%d\" % reverse_gradient.num_calls\n",
        "\n",
        "    @ops.RegisterGradient(grad_name)\n",
        "    def _flip_gradients(op,grad):\n",
        "        return [tf.negative(grad) * hp_lambda]\n",
        "\n",
        "    g = K.get_session().graph\n",
        "    with g.gradient_override_map({'Identity': grad_name}):\n",
        "        y = tf.identity(X)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "class GradientReversal(Layer):\n",
        "    \"\"\"Layer that flips the sign of gradient during training.\"\"\"\n",
        "\n",
        "    def __init__(self, hp_lambda, **kwargs):\n",
        "        super(GradientReversal, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.hp_lambda = hp_lambda\n",
        "\n",
        "    @staticmethod\n",
        "    def get_output_shape_for(input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.trainable_weights = []\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        return reverse_gradient(x, self.hp_lambda)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {}\n",
        "        base_config = super(GradientReversal, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "################################################################################################\n",
        "# MSTGCN Block\n",
        "\n",
        "def MSTGCN_Block(x, k, num_of_chev_filters, num_of_time_filters, time_conv_strides,\n",
        "                 cheb_polynomials, time_conv_kernel, GLalpha, i=0):\n",
        "    '''\n",
        "    packaged Spatial-temporal convolution Block\n",
        "    -------\n",
        "    x: input data;\n",
        "    k: k-order cheb GCN\n",
        "    i: block number\n",
        "    '''\n",
        "\n",
        "    # temporal attention\n",
        "    temporal_Att = TemporalAttention()(x)\n",
        "    x_TAtt = Lambda(reshape_dot, name='reshape_dot'+str(i))([x, temporal_Att])\n",
        "\n",
        "    # spatial attention\n",
        "    spatial_Att = SpatialAttention()(x_TAtt)\n",
        "\n",
        "    # multi-view GCN\n",
        "    S = Graph_Learn(alpha=GLalpha)(x)\n",
        "    S = Dropout(0.3)(S)\n",
        "    spatial_gcn_GL = cheb_conv_with_Att_GL(num_of_filters=num_of_chev_filters, k=k)([x, spatial_Att, S])\n",
        "    spatial_gcn_SD = cheb_conv_with_Att_static(num_of_filters=num_of_chev_filters, k=k,\n",
        "                                               cheb_polynomials=cheb_polynomials)([x, spatial_Att])\n",
        "\n",
        "    # temporal convolution\n",
        "    time_conv_output_GL = layers.Conv2D(\n",
        "        filters=num_of_time_filters,\n",
        "        kernel_size=(time_conv_kernel, 1),\n",
        "        padding='same',\n",
        "        strides=(1, time_conv_strides))(spatial_gcn_GL)\n",
        "\n",
        "    time_conv_output_SD = layers.Conv2D(\n",
        "        filters=num_of_time_filters,\n",
        "        kernel_size=(time_conv_kernel, 1),\n",
        "        padding='same',\n",
        "        strides=(1, time_conv_strides))(spatial_gcn_SD)\n",
        "\n",
        "    # LayerNorm\n",
        "    end_output_GL = Lambda(LayerNorm, name='layer_norm' + str(2*i))(time_conv_output_GL)\n",
        "    end_output_SD = Lambda(LayerNorm, name='layer_norm' + str(2*i+1))(time_conv_output_SD)\n",
        "    return end_output_GL, end_output_SD\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "################################################################################################\n",
        "# MSTGCN\n",
        "\n",
        "def build_MSTGCN(k, num_of_chev_filters, num_of_time_filters, time_conv_strides, cheb_polynomials,\n",
        "                 time_conv_kernel, sample_shape, num_block, dense_size, opt, GLalpha,\n",
        "                 regularizer, dropout, lambda_reversal, num_classes=5, num_domain=9):\n",
        "\n",
        "    # Input:  (*, num_of_timesteps, num_of_vertices, num_of_features)\n",
        "    data_layer = layers.Input(shape=sample_shape, name='Input_Layer')\n",
        "\n",
        "    # MSTGCN_Block\n",
        "    block_out_GL, block_out_SD = MSTGCN_Block(data_layer, k, num_of_chev_filters, num_of_time_filters,\n",
        "                                              time_conv_strides, cheb_polynomials, time_conv_kernel, GLalpha)\n",
        "    for i in range(1, num_block):\n",
        "        block_out_GL, block_out_SD = MSTGCN_Block(block_out_GL, k, num_of_chev_filters, num_of_time_filters,\n",
        "                                                  time_conv_strides, cheb_polynomials, time_conv_kernel, GLalpha, i)\n",
        "    block_out = layers.concatenate([block_out_GL, block_out_SD])\n",
        "    block_out = layers.Flatten()(block_out)\n",
        "\n",
        "    # dropout\n",
        "    if dropout != 0:\n",
        "        block_out = layers.Dropout(dropout)(block_out)\n",
        "\n",
        "    # Global dense layer\n",
        "    for size in dense_size:\n",
        "        dense_out = layers.Dense(size)(block_out)\n",
        "\n",
        "    # softmax classification\n",
        "    softmax = layers.Dense(num_classes,\n",
        "                           activation='softmax',\n",
        "                           kernel_regularizer=regularizer,\n",
        "                           name='Label')(dense_out)\n",
        "\n",
        "    # GRL & G_d\n",
        "    flip_layer = GradientReversal(lambda_reversal)\n",
        "    G_d_in = flip_layer(block_out)\n",
        "    for size in dense_size:\n",
        "        G_d_out = layers.Dense(size)(G_d_in)\n",
        "    G_d_out = layers.Dense(units=num_domain,\n",
        "                           activation='softmax',\n",
        "                           name='Domain')(G_d_out)\n",
        "\n",
        "    # training model (with GRL & G_d)\n",
        "    model = models.Model(inputs=data_layer, outputs=[softmax, G_d_out])\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['acc'],\n",
        "    )\n",
        "    # testing model (without GRL & G_d)\n",
        "    pre_model = models.Model(inputs=data_layer, outputs=softmax)\n",
        "    pre_model.compile(\n",
        "        optimizer=opt,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['acc'],\n",
        "    )\n",
        "    return model, pre_model\n",
        "\n",
        "\n",
        "def build_MSTGCN_test():\n",
        "    # an example to test\n",
        "    cheb_k = 3\n",
        "    num_of_chev_filters = 10\n",
        "    num_of_time_filters = 10\n",
        "    time_conv_strides = 1\n",
        "    time_conv_kernel = 3\n",
        "    dense_size = np.array([64, 32])\n",
        "    cheb_polynomials = [np.random.rand(26, 26), np.random.rand(26, 26), np.random.rand(26, 26)]\n",
        "\n",
        "    model = build_MSTGCN(cheb_k, num_of_chev_filters, num_of_time_filters, time_conv_strides, cheb_polynomials,\n",
        "                         time_conv_kernel, sample_shape=(5, 26, 9), num_block=1, dense_size=dense_size,\n",
        "                         opt='adam', useGL=True, GLalpha=0.0001, regularizer=None, dropout=0.0)\n",
        "    model.summary()\n",
        "    model.save('MSTGCN_build_test.h5')\n",
        "    print(\"save ok\")\n",
        "    return model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "vz5_V9x1G2dr",
        "outputId": "90066170-71ad-4bc6-a340-f977375b904b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.layers.core'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-dd50ad23ce16>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.core'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "import shutil\n",
        "import gc\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import keras.backend.tensorflow_backend as KTF\n",
        "\n",
        "from model.MSTGCN import build_MSTGCN\n",
        "from model.DataGenerator import DominGenerator\n",
        "from model.Utils import *\n",
        "\n",
        "print(128 * '#')\n",
        "print('Start to train MSTGCN.')\n",
        "\n",
        "# # 1. Get configuration\n",
        "\n",
        "# ## 1.1. Read .config file\n",
        "\n",
        "# command line parameters -c -g\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"-c\", type = str, help = \"configuration file\", required = True)\n",
        "parser.add_argument(\"-g\", type = str, help = \"GPU number to use, set '-1' to use CPU\", required = True)\n",
        "args = parser.parse_args()\n",
        "Path, _, cfgTrain, cfgModel = ReadConfig(args.c)\n",
        "\n",
        "# set GPU number or use CPU only\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.g\n",
        "if args.g != \"-1\":\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth=True\n",
        "    sess = tf.Session(config=config)\n",
        "    KTF.set_session(sess)\n",
        "    print(\"Use GPU #\"+args.g)\n",
        "else:\n",
        "    print(\"Use CPU only\")\n",
        "\n",
        "# ## 1.2. Analytic parameters\n",
        "\n",
        "# [train] parameters ('_f' means FeatureNet)\n",
        "channels   = int(cfgTrain[\"channels\"])\n",
        "fold       = int(cfgTrain[\"fold\"])\n",
        "context    = int(cfgTrain[\"context\"])\n",
        "num_epochs = int(cfgTrain[\"epoch\"])\n",
        "batch_size = int(cfgTrain[\"batch_size\"])\n",
        "optimizer  = cfgTrain[\"optimizer\"]\n",
        "learn_rate = float(cfgTrain[\"learn_rate\"])\n",
        "lambda_GRL   = float(cfgTrain[\"lambda_GRL\"])\n",
        "\n",
        "# [model] parameters\n",
        "dense_size            = np.array(str.split(cfgModel[\"Globaldense\"],','),dtype=int)\n",
        "GLalpha               = float(cfgModel[\"GLalpha\"])\n",
        "num_of_chev_filters   = int(cfgModel[\"cheb_filters\"])\n",
        "num_of_time_filters   = int(cfgModel[\"time_filters\"])\n",
        "time_conv_strides     = int(cfgModel[\"time_conv_strides\"])\n",
        "time_conv_kernel      = int(cfgModel[\"time_conv_kernel\"])\n",
        "num_block             = int(cfgModel[\"num_block\"])\n",
        "cheb_k                = int(cfgModel[\"cheb_k\"])\n",
        "l1                    = float(cfgModel[\"l1\"])\n",
        "l2                    = float(cfgModel[\"l2\"])\n",
        "dropout               = float(cfgModel[\"dropout\"])\n",
        "\n",
        "# ## 1.3. Parameter check and enable\n",
        "\n",
        "# Create save pathand copy .config to it\n",
        "if not os.path.exists(Path['Save']):\n",
        "    os.makedirs(Path['Save'])\n",
        "shutil.copyfile(args.c, Path['Save']+\"last.config\")\n",
        "\n",
        "\n",
        "# # 2. Read data and process data\n",
        "\n",
        "# ## 2.1. Read data\n",
        "# Each fold corresponds to one subject's data (ISRUC-S3 dataset)\n",
        "ReadList = np.load(Path['data'], allow_pickle=True)\n",
        "Fold_Num   = ReadList['Fold_len']    # Num of samples of each fold\n",
        "\n",
        "# ## 2.2. Read adjacency matrix\n",
        "# Prepare Chebyshev polynomial of G_DC\n",
        "Dis_Conn = np.load(Path['disM'], allow_pickle=True)  # shape:[V,V]\n",
        "L_DC = scaled_Laplacian(Dis_Conn)                    # Calculate laplacian matrix\n",
        "cheb_poly_DC = cheb_polynomial(L_DC, cheb_k)         # K-order Chebyshev polynomial\n",
        "\n",
        "print(\"Read data successfully\")\n",
        "Fold_Num_c  = Fold_Num + 1 - context\n",
        "print('Number of samples: ',np.sum(Fold_Num), '(with context:', np.sum(Fold_Num_c), ')')\n",
        "\n",
        "# ## 2.3. Build kFoldGenerator or DominGenerator\n",
        "Dom_Generator = DominGenerator(Fold_Num_c)\n",
        "\n",
        "\n",
        "# # 3. Model training (cross validation)\n",
        "\n",
        "# k-fold cross validation\n",
        "all_scores = []\n",
        "for i in range(fold):\n",
        "    print(128*'_')\n",
        "    print('Fold #', i)\n",
        "\n",
        "    # Instantiation optimizer\n",
        "    opt = Instantiation_optim(optimizer, learn_rate)\n",
        "    # Instantiation l1, l2 regularizer\n",
        "    regularizer = Instantiation_regularizer(l1, l2)\n",
        "\n",
        "    # get i th-fold feature and label\n",
        "    Features = np.load(Path['Save']+'Feature_'+str(i)+'.npz', allow_pickle=True)\n",
        "    train_feature = Features['train_feature']\n",
        "    val_feature   = Features['val_feature']\n",
        "    train_targets = Features['train_targets']\n",
        "    val_targets   = Features['val_targets']\n",
        "\n",
        "    ## Use the feature to train MSTGCN\n",
        "\n",
        "    print('Feature',train_feature.shape,val_feature.shape)\n",
        "    train_feature, train_targets  = AddContext_MultiSub(train_feature, train_targets,\n",
        "                                                        np.delete(Fold_Num.copy(), i), context, i)\n",
        "    val_feature, val_targets      = AddContext_SingleSub(val_feature, val_targets, context)\n",
        "    train_domin, val_domin = Dom_Generator.getFold(i)\n",
        "\n",
        "    sample_shape = (val_feature.shape[1:])\n",
        "\n",
        "    print('Feature with context:',train_feature.shape, val_feature.shape)\n",
        "    model, model_p = build_MSTGCN(cheb_k, num_of_chev_filters, num_of_time_filters, time_conv_strides, cheb_poly_DC,\n",
        "                                  time_conv_kernel, sample_shape, num_block, dense_size, opt, GLalpha, regularizer,\n",
        "                                  dropout, lambda_GRL, num_classes=5, num_domain=9) # '_p' model is without GRL\n",
        "\n",
        "    # train\n",
        "    history = model.fit(\n",
        "        x = train_feature,\n",
        "        y = [train_targets,train_domin],\n",
        "        epochs = num_epochs,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = True,\n",
        "        validation_data = (val_feature, [val_targets,val_domin]),\n",
        "        verbose = 2,\n",
        "        callbacks=[keras.callbacks.ModelCheckpoint(Path['Save']+'MSTGCN_Best_'+str(i)+'.h5',\n",
        "                                                   monitor='val_Label_acc',\n",
        "                                                   verbose=0,\n",
        "                                                   save_best_only=True,\n",
        "                                                   save_weights_only=False,\n",
        "                                                   mode='auto',\n",
        "                                                   period=1 )])\n",
        "\n",
        "    # save the final model\n",
        "    model.save(Path['Save']+'MSTGCN_Final_'+str(i)+'.h5')\n",
        "\n",
        "    # Save training information\n",
        "    if i==0:\n",
        "        fit_loss = np.array(history.history['loss'])*Fold_Num_c[i]\n",
        "        fit_acc = np.array(history.history['Label_acc'])*Fold_Num_c[i]\n",
        "        fit_val_loss = np.array(history.history['val_loss'])*Fold_Num_c[i]\n",
        "        fit_val_acc = np.array(history.history['val_Label_acc'])*Fold_Num_c[i]\n",
        "    else:\n",
        "        fit_loss = fit_loss+np.array(history.history['loss'])*Fold_Num_c[i]\n",
        "        fit_acc = fit_acc+np.array(history.history['Label_acc'])*Fold_Num_c[i]\n",
        "        fit_val_loss = fit_val_loss+np.array(history.history['val_loss'])*Fold_Num_c[i]\n",
        "        fit_val_acc = fit_val_acc+np.array(history.history['val_Label_acc'])*Fold_Num_c[i]\n",
        "\n",
        "    saveFile = open(Path['Save'] + \"Result_MSTGCN.txt\", 'a+')\n",
        "    print('Fold #'+str(i), file=saveFile)\n",
        "    print(history.history, file=saveFile)\n",
        "    saveFile.close()\n",
        "\n",
        "    # Fold finish\n",
        "    keras.backend.clear_session()\n",
        "    del model, model_p, train_feature, train_targets, val_feature, val_targets\n",
        "    gc.collect()\n",
        "\n",
        "# # 4. Final results\n",
        "\n",
        "# Average training performance\n",
        "fit_acc      = fit_acc/np.sum(Fold_Num_c)\n",
        "fit_loss     = fit_loss/np.sum(Fold_Num_c)\n",
        "fit_val_loss = fit_val_loss/np.sum(Fold_Num_c)\n",
        "fit_val_acc  = fit_val_acc/np.sum(Fold_Num_c)\n",
        "\n",
        "# Draw ACC / loss curve and save\n",
        "VariationCurve(fit_acc, fit_val_acc, 'Acc', Path['Save'], figsize=(9, 6))\n",
        "VariationCurve(fit_loss, fit_val_loss, 'Loss', Path['Save'], figsize=(9, 6))\n",
        "\n",
        "saveFile = open(Path['Save'] + \"Result_MSTGCN.txt\", 'a+')\n",
        "print(history.history, file=saveFile)\n",
        "saveFile.close()\n",
        "\n",
        "print(128 * '_')\n",
        "print('End of training MSTGCN.')\n",
        "print(128 * '#')\n"
      ],
      "metadata": {
        "id": "h8PzpWYwGxcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}