{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahtesham-Ibne-Mostafa/Escape/blob/main/ISRUC_SLEEP_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cITK8i2liUMw",
        "outputId": "250872eb-d2ab-4970-9697-337e4aa9b4ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing Functions"
      ],
      "metadata": {
        "id": "ooeqcNJgP8Vo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eZQRW5ArBw2r",
        "outputId": "c542a2c5-10fa-472e-8478-1d2ec7317a8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\noutput:\\n    save to $path_output/ISRUC_S3.npz:\\n        Fold_data:  [k-fold] list, each element is [N,V,T]\\n        Fold_label: [k-fold] list, each element is [N,C]\\n        Fold_len:   [k-fold] list\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "import scipy.io as scio\n",
        "from os import path\n",
        "from scipy import signal\n",
        "\n",
        "path_Extracted = '/content/drive/MyDrive/isruc-sleep-data/ExtractedChannels'\n",
        "path_RawData   = '/content/drive/MyDrive/isruc-sleep-data/RawData'\n",
        "path_output    = '/content/drive/MyDrive/isruc-sleep-data/ISRUC_S3'\n",
        "channels = ['C3_A2', 'C4_A1', 'F3_A2', 'F4_A1', 'O1_A2', 'O2_A1',\n",
        "            'LOC_A2', 'ROC_A1','X1', 'X2']\n",
        "\n",
        "\n",
        "def read_psg(path_Extracted, sub_id, channels, resample=3000):\n",
        "    psg = scio.loadmat(path.join(path_Extracted, 'subject%d.mat' % (sub_id)))\n",
        "    psg_use = []\n",
        "    for c in channels:\n",
        "        psg_use.append(\n",
        "            np.expand_dims(signal.resample(psg[c], resample, axis=-1), 1))\n",
        "    psg_use = np.concatenate(psg_use, axis=1)\n",
        "    return psg_use\n",
        "\n",
        "\n",
        "def read_label(path_RawData, sub_id, ignore=30):\n",
        "    label = []\n",
        "    with open(path.join(path_RawData, '%d/%d_1.txt' % (sub_id, sub_id))) as f:\n",
        "        s = f.readline()\n",
        "        while True:\n",
        "            a = s.replace('\\n', '')\n",
        "            label.append(int(a))\n",
        "            s = f.readline()\n",
        "            if s == '' or s == '\\n':\n",
        "                break\n",
        "    return np.array(label[:-ignore])\n",
        "\n",
        "\n",
        "'''\n",
        "output:\n",
        "    save to $path_output/ISRUC_S3.npz:\n",
        "        Fold_data:  [k-fold] list, each element is [N,V,T]\n",
        "        Fold_label: [k-fold] list, each element is [N,C]\n",
        "        Fold_len:   [k-fold] list\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "jMDO3WwqQCtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fold_label = []\n",
        "fold_psg = []\n",
        "fold_len = []\n",
        "\n",
        "for sub in range(1, 2):\n",
        "    print('Read subject', sub)\n",
        "    label = read_label(path_RawData, sub)\n",
        "    psg = read_psg(path_Extracted, sub, channels)\n",
        "    print('Subject', sub, ':', label.shape, psg.shape)\n",
        "    assert len(label) == len(psg)\n",
        "\n",
        "    # in ISRUC, 0-Wake, 1-N1, 2-N2, 3-N3, 5-REM\n",
        "    label[label==5] = 4  # make 4 correspond to REM\n",
        "    fold_label.append(np.eye(5)[label])\n",
        "    fold_psg.append(psg)\n",
        "    fold_len.append(len(label))\n",
        "print('Preprocess over.')\n",
        "\n",
        "np.savez(path.join(path_output, 'ISRUC_S3.npz'),\n",
        "    Fold_data = fold_psg,\n",
        "    Fold_label = fold_label,\n",
        "    Fold_len = fold_len\n",
        ")\n",
        "print('Saved to', path.join(path_output, 'ISRUC_S3.npz'))\n"
      ],
      "metadata": {
        "id": "3b_2DE4CDmsn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91285bc5-d3da-4978-b80c-d80928eea04c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read subject 1\n",
            "Subject 1 : (924,) (924, 10, 3000)\n",
            "Preprocess over.\n",
            "Saved to /content/drive/MyDrive/isruc-sleep-data/ISRUC_S3/ISRUC_S3.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Generator Model"
      ],
      "metadata": {
        "id": "XUdvJJndaQ8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class kFoldGenerator:\n",
        "    '''\n",
        "    Data Generator\n",
        "    '''\n",
        "    def __init__(self, x, y):\n",
        "        if len(x) != len(y):\n",
        "            assert False, 'Data generator: Length of x or y is not equal.'\n",
        "        self.k = len(x)\n",
        "        self.x_list = x\n",
        "        self.y_list = y\n",
        "\n",
        "    def getFold(self, i):\n",
        "        train_data = None\n",
        "        train_targets = None\n",
        "        val_data = None\n",
        "        val_targets = None\n",
        "\n",
        "        for p in range(self.k):\n",
        "            print('-------------',p,i)\n",
        "            if p != i:\n",
        "                if train_data is None:\n",
        "                    train_data = self.x_list[p]\n",
        "                    train_targets = self.y_list[p]\n",
        "                else:\n",
        "                    train_data = np.concatenate((train_data, self.x_list[p]))\n",
        "                    train_targets = np.concatenate((train_targets, self.y_list[p]))\n",
        "            else:\n",
        "                val_data = self.x_list[p]\n",
        "                val_targets = self.y_list[p]\n",
        "\n",
        "        return train_data, train_targets, val_data, val_targets\n",
        "\n",
        "\n",
        "    # Get all data x\n",
        "    def getX(self):\n",
        "        All_X = self.x_list[0]\n",
        "        for i in range(1, self.k):\n",
        "            All_X = np.append(All_X, self.x_list[i], axis=0)\n",
        "        return All_X\n",
        "\n",
        "    # Get all label y (one-hot)\n",
        "    def getY(self):\n",
        "        All_Y = self.y_list[0][2:-2]\n",
        "        for i in range(1, self.k):\n",
        "            All_Y = np.append(All_Y, self.y_list[i][2:-2], axis=0)\n",
        "        return All_Y\n",
        "\n",
        "    # Get all label y (int)\n",
        "    def getY_int(self):\n",
        "        All_Y = self.getY()\n",
        "        return np.argmax(All_Y, axis=1)\n",
        "\n",
        "\n",
        "class DominGenerator():\n",
        "    '''\n",
        "    Domin Generator\n",
        "    '''\n",
        "    k = -1       # the fold number\n",
        "    l_list = []  # length of each domin\n",
        "    d_list = []  # d list with length=k\n",
        "\n",
        "    # Initializate\n",
        "    def __init__(self, len_list):\n",
        "        self.l_list = len_list\n",
        "        self.k = len(len_list)\n",
        "\n",
        "    # Get i-th fold\n",
        "    def getFold(self, i):\n",
        "        isFirst = True\n",
        "        isFirstVal = True\n",
        "        j = 0   #1~9\n",
        "        ii = 0  #1~10\n",
        "        for l in self.l_list:\n",
        "            if ii != i:\n",
        "                a = np.zeros((l, 9), dtype=int)\n",
        "                a[:, j] = 1\n",
        "                if isFirst:\n",
        "                    train_domin = a\n",
        "                    isFirst = False\n",
        "                else:\n",
        "                    train_domin = np.concatenate((train_domin, a))\n",
        "                j += 1\n",
        "            else:\n",
        "                if isFirstVal:\n",
        "                    val_domin = np.zeros((l, 9), dtype=int)\n",
        "                    isFirstVal = False\n",
        "                else:\n",
        "                    a = np.zeros((l, 9), dtype=int)\n",
        "                    val_domin = np.concatenate((val_domin, a))\n",
        "            ii += 1\n",
        "        return train_domin, val_domin\n"
      ],
      "metadata": {
        "id": "hjFwFqRkFx8j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FeatureNet Model\n"
      ],
      "metadata": {
        "id": "-J3GPoiUakZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv1D, Dense, Dropout, MaxPool1D, Activation\n",
        "from keras.layers import Flatten, Reshape, TimeDistributed, BatchNormalization\n",
        "\n",
        "'''\n",
        "A Feature Extractor Network\n",
        "'''\n",
        "\n",
        "def build_FeatureNet(opt, channels=10, time_second=30, freq=100):\n",
        "    activation = tf.nn.relu\n",
        "    padding = 'same'\n",
        "\n",
        "    ######### Input ########\n",
        "    input_signal = Input(shape=(time_second * freq, 1), name='input_signal')\n",
        "\n",
        "    ######### CNNs with small filter size at the first layer #########\n",
        "    cnn0 = Conv1D(kernel_size=50,\n",
        "                  filters=32,\n",
        "                  strides=6,\n",
        "                  kernel_regularizer=keras.regularizers.l2(0.001))\n",
        "    s = cnn0(input_signal)\n",
        "    s = BatchNormalization()(s)\n",
        "    s = Activation(activation=activation)(s)\n",
        "    cnn1 = MaxPool1D(pool_size=16, strides=16)\n",
        "    s = cnn1(s)\n",
        "    cnn2 = Dropout(0.5)\n",
        "    s = cnn2(s)\n",
        "    cnn3 = Conv1D(kernel_size=8, filters=64, strides=1, padding=padding)\n",
        "    s = cnn3(s)\n",
        "    s = BatchNormalization()(s)\n",
        "    s = Activation(activation=activation)(s)\n",
        "    cnn4 = Conv1D(kernel_size=8, filters=64, strides=1, padding=padding)\n",
        "    s = cnn4(s)\n",
        "    s = BatchNormalization()(s)\n",
        "    s = Activation(activation=activation)(s)\n",
        "    cnn5 = Conv1D(kernel_size=8, filters=64, strides=1, padding=padding)\n",
        "    s = cnn5(s)\n",
        "    s = BatchNormalization()(s)\n",
        "    s = Activation(activation=activation)(s)\n",
        "    cnn6 = MaxPool1D(pool_size=8, strides=8)\n",
        "    s = cnn6(s)\n",
        "    cnn7 = Reshape((int(s.shape[1]) * int(s.shape[2]), ))  # Flatten\n",
        "    s = cnn7(s)\n",
        "\n",
        "    ######### CNNs with large filter size at the first layer #########\n",
        "    cnn8 = Conv1D(kernel_size=400,\n",
        "                  filters=64,\n",
        "                  strides=50,\n",
        "                  kernel_regularizer=keras.regularizers.l2(0.001))\n",
        "    l = cnn8(input_signal)\n",
        "    l = BatchNormalization()(l)\n",
        "    l = Activation(activation=activation)(l)\n",
        "    cnn9 = MaxPool1D(pool_size=8, strides=8)\n",
        "    l = cnn9(l)\n",
        "    cnn10 = Dropout(0.5)\n",
        "    l = cnn10(l)\n",
        "    cnn11 = Conv1D(kernel_size=6, filters=64, strides=1, padding=padding)\n",
        "    l = cnn11(l)\n",
        "    l = BatchNormalization()(l)\n",
        "    l = Activation(activation=activation)(l)\n",
        "    cnn12 = Conv1D(kernel_size=6, filters=64, strides=1, padding=padding)\n",
        "    l = cnn12(l)\n",
        "    l = BatchNormalization()(l)\n",
        "    l = Activation(activation=activation)(l)\n",
        "    cnn13 = Conv1D(kernel_size=6, filters=64, strides=1, padding=padding)\n",
        "    l = cnn13(l)\n",
        "    l = BatchNormalization()(l)\n",
        "    l = Activation(activation=activation)(l)\n",
        "    cnn14 = MaxPool1D(pool_size=4, strides=4)\n",
        "    l = cnn14(l)\n",
        "    cnn15 = Reshape((int(l.shape[1]) * int(l.shape[2]), ))\n",
        "    l = cnn15(l)\n",
        "\n",
        "    feature = keras.layers.concatenate([s, l])\n",
        "\n",
        "    fea_part = Model(input_signal, feature)\n",
        "\n",
        "    ##################################################\n",
        "\n",
        "    input = Input(shape=(channels, time_second * freq), name='input_signal')\n",
        "    reshape = Reshape((channels, time_second * freq, 1))  # Flatten\n",
        "    input_re = reshape(input)\n",
        "    fea_all = TimeDistributed(fea_part)(input_re)\n",
        "\n",
        "    merged = Flatten()(fea_all)\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    merged = Dense(64)(merged)\n",
        "    merged = Dense(5)(merged)\n",
        "\n",
        "    fea_softmax = Activation(activation='softmax')(merged)\n",
        "\n",
        "    # FeatureNet with softmax\n",
        "    fea_model = Model(input, fea_softmax)\n",
        "    fea_model.compile(optimizer=opt,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['acc'])\n",
        "\n",
        "    # FeatureNet without softmax\n",
        "    pre_model = Model(input, fea_all)\n",
        "    pre_model.compile(optimizer=opt,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['acc'])\n",
        "\n",
        "    return fea_model, pre_model\n"
      ],
      "metadata": {
        "id": "BLwrQK63F9OA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils Model\n"
      ],
      "metadata": {
        "id": "8Y3yLS9pausT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import configparser\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from scipy.sparse.linalg import eigs\n",
        "import keras\n",
        "\n",
        "\n",
        "##########################################################################################\n",
        "# Read configuration file ################################################################\n",
        "\n",
        "def ReadConfig(configfile):\n",
        "    config = configparser.ConfigParser()\n",
        "    print('Config: ', configfile)\n",
        "    config.read(configfile)\n",
        "    cfgPath = config['path']\n",
        "    cfgFeat = config['feature']\n",
        "    cfgTrain = config['train']\n",
        "    cfgModel = config['model']\n",
        "    return cfgPath, cfgFeat, cfgTrain, cfgModel\n",
        "\n",
        "##########################################################################################\n",
        "# Add context to the origin data and label ###############################################\n",
        "\n",
        "def AddContext_MultiSub(x, y, Fold_Num, context, i):\n",
        "    '''\n",
        "    input:\n",
        "        x       : [N,V,F];\n",
        "        y       : [N,C]; (C:num_of_classes)\n",
        "        Fold_Num: [kfold];\n",
        "        context : int;\n",
        "        i       : int (i-th fold)\n",
        "    return:\n",
        "        x with contexts. [N',V,F]\n",
        "    '''\n",
        "    cut = context // 2\n",
        "    fold = Fold_Num.copy()\n",
        "    fold = np.delete(fold, -1)\n",
        "    id_del = np.concatenate([np.cumsum(fold) - i for i in range(1, context)])\n",
        "    id_del = np.sort(id_del)\n",
        "\n",
        "    x_c = np.zeros([x.shape[0] - 2 * cut, context, x.shape[1], x.shape[2]], dtype=float)\n",
        "    for j in range(cut, x.shape[0] - cut):\n",
        "        x_c[j - cut] = x[j - cut:j + cut + 1]\n",
        "\n",
        "    x_c = np.delete(x_c, id_del, axis=0)\n",
        "    y_c = np.delete(y[cut: -cut], id_del, axis=0)\n",
        "    return x_c, y_c\n",
        "\n",
        "def AddContext_SingleSub(x, y, context):\n",
        "    cut = int(context / 2)\n",
        "    x_c = np.zeros([x.shape[0] - 2 * cut, context, x.shape[1], x.shape[2]], dtype=float)\n",
        "    for i in range(cut, x.shape[0] - cut):\n",
        "        x_c[i - cut] = x[i - cut:i + cut + 1]\n",
        "    y_c = y[cut:-cut]\n",
        "    return x_c, y_c\n",
        "\n",
        "##########################################################################################\n",
        "# Instantiation operation ################################################################\n",
        "\n",
        "def Instantiation_optim(name, lr):\n",
        "    if   name==\"adam\":\n",
        "        opt = keras.optimizers.Adam(lr=lr)\n",
        "    elif name==\"RMSprop\":\n",
        "        opt = keras.optimizers.RMSprop(lr=lr)\n",
        "    elif name==\"SGD\":\n",
        "        opt = keras.optimizers.SGD(lr=lr)\n",
        "    else:\n",
        "        assert False,'Config: check optimizer, may be not implemented.'\n",
        "    return opt\n",
        "\n",
        "def Instantiation_regularizer(l1, l2):\n",
        "    if   l1!=0 and l2!=0:\n",
        "        regularizer = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
        "    elif l1!=0 and l2==0:\n",
        "        regularizer = keras.regularizers.l1(l1)\n",
        "    elif l1==0 and l2!=0:\n",
        "        regularizer = keras.regularizers.l2(l2)\n",
        "    else:\n",
        "        regularizer = None\n",
        "    return regularizer\n",
        "\n",
        "##########################################################################################\n",
        "# Print score between Ytrue and Ypred ####################################################\n",
        "\n",
        "def PrintScore(true, pred, savePath=None, average='macro'):\n",
        "    # savePath=None -> console, else to Result.txt\n",
        "    if savePath == None:\n",
        "        saveFile = None\n",
        "    else:\n",
        "        saveFile = open(savePath + \"Result.txt\", 'a+')\n",
        "    # Main scores\n",
        "    F1 = metrics.f1_score(true, pred, average=None)\n",
        "    print(\"Main scores:\")\n",
        "    print('Acc\\tF1S\\tKappa\\tF1_W\\tF1_N1\\tF1_N2\\tF1_N3\\tF1_R', file=saveFile)\n",
        "    print('%.4f\\t%.4f\\t%.4f\\t%.4f\\t%.4f\\t%.4f\\t%.4f\\t%.4f' %\n",
        "          (metrics.accuracy_score(true, pred),\n",
        "           metrics.f1_score(true, pred, average=average),\n",
        "           metrics.cohen_kappa_score(true, pred),\n",
        "           F1[0], F1[1], F1[2], F1[3], F1[4]),\n",
        "          file=saveFile)\n",
        "    # Classification report\n",
        "    print(\"\\nClassification report:\", file=saveFile)\n",
        "    print(metrics.classification_report(true, pred,\n",
        "                                        target_names=['Wake','N1','N2','N3','REM'],\n",
        "                                        digits=4), file=saveFile)\n",
        "    # Confusion matrix\n",
        "    print('Confusion matrix:', file=saveFile)\n",
        "    print(metrics.confusion_matrix(true,pred), file=saveFile)\n",
        "    # Overall scores\n",
        "    print('\\n    Accuracy\\t',metrics.accuracy_score(true,pred), file=saveFile)\n",
        "    print(' Cohen Kappa\\t',metrics.cohen_kappa_score(true,pred), file=saveFile)\n",
        "    print('    F1-Score\\t',metrics.f1_score(true,pred,average=average), '\\tAverage =',average, file=saveFile)\n",
        "    print('   Precision\\t',metrics.precision_score(true,pred,average=average), '\\tAverage =',average, file=saveFile)\n",
        "    print('      Recall\\t',metrics.recall_score(true,pred,average=average), '\\tAverage =',average, file=saveFile)\n",
        "    if savePath != None:\n",
        "        saveFile.close()\n",
        "    return\n",
        "\n",
        "##########################################################################################\n",
        "# Print confusion matrix and save ########################################################\n",
        "\n",
        "def ConfusionMatrix(y_true, y_pred, classes, savePath, title=None, cmap=plt.cm.Blues):\n",
        "    if not title:\n",
        "        title = 'Confusion matrix'\n",
        "    # Compute confusion matrix\n",
        "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
        "    cm_n=cm\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    fig, ax = plt.subplots(figsize=(5, 4))\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation_mode=\"anchor\")\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j]*100,'.2f')+'%\\n'+format(cm_n[i, j],'d'),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(savePath+title+\".png\")\n",
        "    plt.show()\n",
        "    return ax\n",
        "\n",
        "##########################################################################################\n",
        "# Draw ACC / loss curve and save #########################################################\n",
        "\n",
        "def VariationCurve(fit,val,yLabel,savePath,figsize=(9, 6)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.plot(range(1,len(fit)+1), fit,label='Train')\n",
        "    plt.plot(range(1,len(val)+1), val, label='Val')\n",
        "    plt.title('Model ' + yLabel)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(yLabel)\n",
        "    plt.legend()\n",
        "    plt.savefig(savePath + 'Model_' + yLabel + '.png')\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "# compute \\tilde{L}\n",
        "\n",
        "def scaled_Laplacian(W):\n",
        "    '''\n",
        "    compute \\tilde{L}\n",
        "    ----------\n",
        "    Parameters\n",
        "    W: np.ndarray, shape is (N, N), N is the num of vertices\n",
        "    ----------\n",
        "    Returns\n",
        "    scaled_Laplacian: np.ndarray, shape (N, N)\n",
        "    '''\n",
        "    assert W.shape[0] == W.shape[1]\n",
        "    D = np.diag(np.sum(W, axis = 1))\n",
        "    L = D - W\n",
        "    lambda_max = eigs(L, k = 1, which = 'LR')[0].real\n",
        "    return (2 * L) / lambda_max - np.identity(W.shape[0])\n",
        "\n",
        "##########################################################################################\n",
        "# compute a list of chebyshev polynomials from T_0 to T_{K-1} ############################\n",
        "\n",
        "def cheb_polynomial(L_tilde, K):\n",
        "    '''\n",
        "    compute a list of chebyshev polynomials from T_0 to T_{K-1}\n",
        "    ----------\n",
        "    Parameters\n",
        "    L_tilde: scaled Laplacian, np.ndarray, shape (N, N)\n",
        "    K: the maximum order of chebyshev polynomials\n",
        "    ----------\n",
        "    Returns\n",
        "    cheb_polynomials: list(np.ndarray), length: K, from T_0 to T_{K-1}\n",
        "    '''\n",
        "    N = L_tilde.shape[0]\n",
        "    cheb_polynomials = np.array([np.identity(N), L_tilde.copy()])\n",
        "    for i in range(2, K):\n",
        "        cheb_polynomials = np.append(\n",
        "            cheb_polynomials,\n",
        "            [2 * L_tilde * cheb_polynomials[i - 1] - cheb_polynomials[i - 2]],\n",
        "            axis=0)\n",
        "    return cheb_polynomials\n"
      ],
      "metadata": {
        "id": "uAVz58TwGBwA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train FeatureNet Dependencies\n"
      ],
      "metadata": {
        "id": "zfq63dINa2Pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import shutil\n",
        "import gc\n",
        "import os\n",
        "import keras\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "tf.enable_eager_execution()\n",
        "#import keras.backend.tensorflow_backend as KTF\n",
        "from tensorflow.keras import backend as KTF\n",
        "# from model.DataGenerator import kFoldGenerator\n",
        "# from model.FeatureNet import build_FeatureNet\n",
        "# from model.Utils import *"
      ],
      "metadata": {
        "id": "WaKo9gxfEv4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc690dd-e121-4b14-8881-097c2cb466b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train FeatureNet"
      ],
      "metadata": {
        "id": "0-r0VO_ka6qN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(128 * '#')\n",
        "print('Start to train FeatureNet.')\n",
        "\n",
        "# # 1. Get configuration\n",
        "\n",
        "# ## 1.1. Read .config file\n",
        "\n",
        "# command line parameters -c -g\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\"-c\", type = str, help = \"configuration file\", required = True)\n",
        "# parser.add_argument(\"-g\", type = str, help = \"GPU number to use, set '-1' to use CPU\", required = True)\n",
        "# args = parser.parse_args()\n",
        "# Path, cfgFeature, _, _ = ReadConfig(args.c)\n",
        "\n",
        "# Assuming it contains the path to your configuration file\n",
        "config_file_path = '/content/drive/MyDrive/isruc-sleep-data/ISRUC.config'\n",
        "\n",
        "# Read the configuration (you'll need to implement 'ReadConfig' function)\n",
        "Path, cfgFeature, _, _ = ReadConfig(config_file_path)\n",
        "\n",
        "# # set GPU number or use CPU only\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.g\n",
        "# if args.g != \"-1\":\n",
        "#     config = tf.ConfigProto()\n",
        "#     config.gpu_options.allow_growth=True\n",
        "#     sess = tf.Session(config=config)\n",
        "#     KTF.set_session(sess)\n",
        "#     print(\"Use GPU #\"+args.g)\n",
        "# else:\n",
        "#     print(\"Use CPU only\")\n",
        "\n",
        "# ## 1.2. Analytic parameters\n",
        "\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# [train] parameters ('_f' means FeatureNet)\n",
        "channels   = int(cfgFeature[\"channels\"])\n",
        "fold       = int(cfgFeature[\"fold\"])\n",
        "num_epochs_f = int(cfgFeature[\"epoch_f\"])\n",
        "batch_size_f = int(cfgFeature[\"batch_size_f\"])\n",
        "optimizer_f  = cfgFeature[\"optimizer_f\"]\n",
        "learn_rate_f = float(cfgFeature[\"learn_rate_f\"])\n",
        "\n",
        "# channels = 10\n",
        "# fold = 10\n",
        "# epoch_f = 80\n",
        "# batch_size_f = 64\n",
        "# optimizer_f = adam\n",
        "# learn_rate_f = 0.0002\n",
        "\n",
        "# ## 1.3. Parameter check and enable\n",
        "\n",
        "# Create save pathand copy .config to it\n",
        "if not os.path.exists(Path['Save']):\n",
        "    os.makedirs(Path['Save'])\n",
        "# shutil.copyfile(args.c, Path['Save']+\"last.config\")\n",
        "\n",
        "\n",
        "# # 2. Read data and process data\n",
        "\n",
        "# ## 2.1. Read data\n",
        "# Each fold corresponds to one subject's data (ISRUC-S3 dataset)\n",
        "ReadList = np.load('/content/drive/MyDrive/isruc-sleep-data/ISRUC_S3/ISRUC_S3.npz', allow_pickle=True)\n",
        "Fold_Num   = ReadList['Fold_len']    # Num of samples of each fold\n",
        "Fold_Data  = ReadList['Fold_data']   # Data of each fold\n",
        "Fold_Label = ReadList['Fold_label']  # Labels of each fold\n",
        "\n",
        "print(\"Read data successfully\")\n",
        "print('Number of samples: ',np.sum(Fold_Num))\n",
        "\n",
        "# ## 2.2. Build kFoldGenerator or DominGenerator\n",
        "DataGenerator = kFoldGenerator(Fold_Data, Fold_Label)\n",
        "\n",
        "\n",
        "# # 3. Model training (cross validation)\n",
        "\n",
        "# k-fold cross validation\n",
        "all_scores = []\n",
        "for i in range(fold):\n",
        "    print(128*'_')\n",
        "    print('Fold #', i)\n",
        "\n",
        "    # Instantiation optimizer\n",
        "    opt_f = Instantiation_optim(optimizer_f, learn_rate_f) # optimizer of FeatureNet\n",
        "\n",
        "    # get i th-fold data\n",
        "    train_data,train_targets,val_data,val_targets = DataGenerator.getFold(i)\n",
        "\n",
        "    ## build FeatureNet & train\n",
        "    featureNet, featureNet_p = build_FeatureNet(opt_f, channels) # '_p' model is without the softmax layer\n",
        "    history_fea = featureNet.fit(\n",
        "        x = train_data,\n",
        "        y = train_targets,\n",
        "        epochs = num_epochs_f,\n",
        "        batch_size = batch_size_f,\n",
        "        shuffle = True,\n",
        "        validation_data = (val_data, val_targets),\n",
        "        verbose = 2,\n",
        "        callbacks=[keras.callbacks.ModelCheckpoint(Path['Save']+'FeatureNet_Best_'+str(i)+'.h5',\n",
        "                                                   monitor='val_acc',\n",
        "                                                   verbose=0,\n",
        "                                                   save_best_only=True,\n",
        "                                                   save_weights_only=False,\n",
        "                                                   mode='auto',\n",
        "                                                   period=1 )])\n",
        "    # Save training information\n",
        "    if i==0:\n",
        "        fit_loss = np.array(history_fea.history['loss'])*Fold_Num[i]\n",
        "        fit_acc = np.array(history_fea.history['acc'])*Fold_Num[i]\n",
        "        fit_val_loss = np.array(history_fea.history['val_loss'])*Fold_Num[i]\n",
        "        fit_val_acc = np.array(history_fea.history['val_acc'])*Fold_Num[i]\n",
        "    else:\n",
        "        fit_loss = fit_loss+np.array(history_fea.history['loss'])*Fold_Num[i]\n",
        "        fit_acc = fit_acc+np.array(history_fea.history['acc'])*Fold_Num[i]\n",
        "        fit_val_loss = fit_val_loss+np.array(history_fea.history['val_loss'])*Fold_Num[i]\n",
        "        fit_val_acc = fit_val_acc+np.array(history_fea.history['val_acc'])*Fold_Num[i]\n",
        "\n",
        "    # load the weights of best performance\n",
        "    featureNet.load_weights(Path['Save']+'FeatureNet_Best_'+str(i)+'.h5')\n",
        "\n",
        "    # get and save the learned feature\n",
        "    train_feature = featureNet_p.predict(train_data)\n",
        "    val_feature = featureNet_p.predict(val_data)\n",
        "    print('Save feature of Fold #' + str(i) + ' to' + Path['Save']+'Feature_'+str(i) + '.npz')\n",
        "    np.savez(Path['Save']+'Feature_'+str(i)+'.npz',\n",
        "        train_feature = train_feature,\n",
        "        val_feature = val_feature,\n",
        "        train_targets = train_targets,\n",
        "        val_targets = val_targets\n",
        "    )\n",
        "\n",
        "    saveFile = open(Path['Save'] + \"Result_FeatureNet.txt\", 'a+')\n",
        "    print('Fold #'+str(i), file=saveFile)\n",
        "    print(history_fea.history, file=saveFile)\n",
        "    saveFile.close()\n",
        "\n",
        "    # Fold finish\n",
        "    keras.backend.clear_session()\n",
        "    del featureNet, featureNet_p, train_data, train_targets, val_data, val_targets, train_feature, val_feature\n",
        "    gc.collect()\n",
        "\n",
        "print(128 * '_')\n",
        "\n",
        "print('End of training FeatureNet.')\n",
        "print(128 * '#')\n"
      ],
      "metadata": {
        "id": "J-sbSQy7EkGP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "4cd859f8-d27d-4ee9-9bdd-6ca31bab73db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################################################################################################################################\n",
            "Start to train FeatureNet.\n",
            "Config:  /content/drive/MyDrive/isruc-sleep-data/ISRUC.config\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read data successfully\n",
            "Number of samples:  924\n",
            "________________________________________________________________________________________________________________________________\n",
            "Fold # 0\n",
            "------------- 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Failed to find data adapter that can handle input: <class 'NoneType'>, <class 'NoneType'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f62de372e660>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m## build FeatureNet & train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mfeatureNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureNet_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_FeatureNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# '_p' model is without the softmax layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     history_fea = featureNet.fit(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1106\u001b[0m             \"Failed to find data adapter that can handle input: {}, {}\".format(\n\u001b[1;32m   1107\u001b[0m                 \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'NoneType'>, <class 'NoneType'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ReadList = np.load('/content/drive/MyDrive/isruc-sleep-data/ISRUC_S3/ISRUC_S3.npz', allow_pickle=True)\n",
        "x  = ReadList['Fold_data']\n",
        "y = ReadList['Fold_label']\n",
        "print(x)"
      ],
      "metadata": {
        "id": "qz4n6NkGs2wM",
        "outputId": "e46dde38-ac7a-4229-9d33-d252e1ba9e22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[-1.58205440e+00 -4.86539654e+00 -5.59333673e+00 ...  8.90383118e-01\n",
            "     1.24807606e+00  1.55469341e+00]\n",
            "   [-5.63348918e-01  1.18659812e+00  1.64288789e+00 ...  2.73122558e+00\n",
            "    -3.94977159e-01 -2.68055270e+00]\n",
            "   [-2.02178742e+00 -4.16303211e+00 -6.07058129e+00 ...  5.40469978e-01\n",
            "     8.12015008e-01  7.31510644e-01]\n",
            "   ...\n",
            "   [-2.39814051e+00 -3.04408887e+00 -4.12245838e+00 ...  1.41362924e+00\n",
            "    -1.43253802e+00 -3.88391945e+00]\n",
            "   [ 2.12341799e+00 -2.11176136e-01 -8.73858268e+00 ... -4.98392148e-01\n",
            "     6.18412458e-01 -3.76233392e-01]\n",
            "   [-9.27995739e-02 -2.16590995e+00  1.16050533e+01 ... -2.26079431e+01\n",
            "    -1.68382820e+01 -1.87199701e+00]]\n",
            "\n",
            "  [[ 8.78100730e-01  1.38298702e+00  1.01512035e+00 ... -1.45230378e+00\n",
            "    -1.08906023e+00 -7.47849688e-02]\n",
            "   [-1.33031073e+00 -9.72898190e-01  4.30508708e-01 ... -4.76946467e-01\n",
            "    -7.52313413e-01  2.49135732e-01]\n",
            "   [ 8.73323145e-02  8.90826594e-01  9.99802581e-01 ... -1.25105974e+00\n",
            "    -8.89074158e-01 -2.34047220e-01]\n",
            "   ...\n",
            "   [-2.53697684e+00 -3.00854663e+00 -1.27220375e+00 ...  8.96167676e-01\n",
            "     8.39049173e-01  1.45876051e+00]\n",
            "   [-3.88740476e-01 -6.82406620e-01  3.66242521e-01 ...  4.91861867e-03\n",
            "    -2.14089888e-01  3.99504476e-01]\n",
            "   [ 3.54472791e+00  4.55735795e+00  2.83068645e+00 ... -1.08768767e+00\n",
            "     2.43834875e-01 -1.21726401e+00]]\n",
            "\n",
            "  [[ 3.14507100e-01  2.39685463e-01  1.13745750e+00 ...  5.39090554e-01\n",
            "     3.39105507e-01  7.00478947e-01]\n",
            "   [-1.38650305e-01 -2.51085384e-01 -8.79761196e-01 ...  1.24755156e-01\n",
            "     4.35817518e-01 -4.54729840e-01]\n",
            "   [ 5.88535504e-02 -2.88901945e-01  2.17058538e-01 ...  5.54735560e-01\n",
            "     6.63131292e-01  1.09116514e+00]\n",
            "   ...\n",
            "   [ 1.03264798e-01  5.13434238e-01 -4.21498631e-01 ...  1.26970059e-01\n",
            "     5.52000268e-01 -9.47980239e-01]\n",
            "   [ 5.73359043e-02 -4.07101593e-01  7.63549746e-01 ... -4.48687542e-02\n",
            "    -1.52615386e-01  5.24526185e-01]\n",
            "   [-1.13656252e+00 -4.47442347e-01 -2.43566404e-02 ...  5.41822210e-02\n",
            "     5.37897943e-02  1.03539972e+00]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-2.83107295e-01 -8.64308630e-01 -5.02722323e-01 ...  1.78614345e+00\n",
            "     1.70155750e+00  1.64176479e+00]\n",
            "   [ 8.11340356e-02  3.09631714e-01  3.36709711e-01 ...  1.10943971e+00\n",
            "     9.04137336e-01  8.37131059e-01]\n",
            "   [ 4.38283962e-01  6.93153361e-02 -2.39359404e-02 ...  1.08006286e+00\n",
            "     1.14447407e+00  1.32710298e+00]\n",
            "   ...\n",
            "   [ 1.16361850e-01  1.32446395e-01  8.01022007e-02 ...  1.40778366e-01\n",
            "     5.00202658e-02  2.76133245e-01]\n",
            "   [ 5.39009104e-01 -3.22291789e-01 -3.85061658e-01 ...  6.17871374e-01\n",
            "    -3.33203174e-01  1.76562825e-03]\n",
            "   [-1.27622525e-01 -3.07912744e-02  2.48200057e-02 ... -1.05146324e+00\n",
            "    -9.21747207e-01 -5.75817868e-01]]\n",
            "\n",
            "  [[-2.37308564e-01  8.26837688e-01  2.70534558e-01 ...  2.98099917e-01\n",
            "    -1.34020139e+00 -3.34560850e+00]\n",
            "   [-7.16334674e-01  3.83465388e-01 -1.72748591e-01 ... -9.96875841e-01\n",
            "    -2.11614149e+00 -3.81940023e+00]\n",
            "   [ 6.35696547e-01  9.33670722e-01  5.32488674e-01 ...  1.81907892e+00\n",
            "     7.38659931e-01 -4.50810639e-01]\n",
            "   ...\n",
            "   [ 3.28535922e-01 -1.23303906e-01 -4.95275640e-01 ...  3.79560833e-01\n",
            "     9.00744110e-01  8.81808735e-01]\n",
            "   [-3.57813638e-01  1.14477043e-01  2.28092148e-01 ...  1.59376186e-01\n",
            "     4.50718551e-01  3.49373575e-02]\n",
            "   [-2.89476741e-01  4.73408350e-01  5.35056410e-01 ...  1.48294684e+00\n",
            "     7.87945454e-02 -9.93201635e-01]]\n",
            "\n",
            "  [[-4.66012030e+00 -3.08798147e+00 -2.68381253e+00 ... -5.07702156e+00\n",
            "    -5.14533900e+00 -6.95426049e+00]\n",
            "   [-4.54025935e+00 -3.68032805e+00 -3.40502657e+00 ... -3.48594374e+00\n",
            "    -3.46299279e+00 -5.06963923e+00]\n",
            "   [-3.28521334e+00 -1.00080083e-01 -7.78467029e-01 ... -8.92912936e+00\n",
            "    -8.12325821e+00 -1.03744806e+01]\n",
            "   ...\n",
            "   [-1.78400492e-01  3.10896294e-01 -3.50015769e-01 ... -1.93155714e-01\n",
            "    -2.54652800e-01 -1.57263856e+00]\n",
            "   [ 2.28705263e-01  1.45586664e-01 -3.24974210e-01 ... -3.33444586e-01\n",
            "    -2.36594506e-01  4.60613590e-01]\n",
            "   [-3.20531440e-01 -7.59421136e-01 -2.09223695e-01 ... -3.74397963e-02\n",
            "     8.36244104e-02 -6.01208587e-02]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "u5egCIlJtCV2",
        "outputId": "eeeb30e9-6f1f-417b-befd-0d9fcae2a57b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSTGCN Model Dependencies"
      ],
      "metadata": {
        "id": "UDRWG4UjhRQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tf-nightly\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.layers import Layer\n",
        "from keras.layers.core import Dropout, Lambda\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.contrib import layers"
      ],
      "metadata": {
        "id": "PlG2ZnJ4HXvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSTGCN Model"
      ],
      "metadata": {
        "id": "U5jFHTQmhVbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Model code of MSTGCN.\n",
        "--------\n",
        "Model input:  (*, T, V, F)\n",
        "    T: num_of_timesteps\n",
        "    V: num_of_vertices\n",
        "    F: num_of_features\n",
        "Model output: (*, 5)\n",
        "'''\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "################################################################################################\n",
        "# Attention Layers\n",
        "\n",
        "class TemporalAttention(Layer):\n",
        "    '''\n",
        "    compute temporal attention scores\n",
        "    --------\n",
        "    Input:  (batch_size, num_of_timesteps, num_of_vertices, num_of_features)\n",
        "    Output: (batch_size, num_of_timesteps, num_of_timesteps)\n",
        "    '''\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TemporalAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_of_timesteps, num_of_vertices, num_of_features = input_shape\n",
        "        self.U_1 = self.add_weight(name='U_1',\n",
        "                                   shape=(num_of_vertices, 1),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_2 = self.add_weight(name='U_2',\n",
        "                                   shape=(num_of_features, num_of_vertices),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_3 = self.add_weight(name='U_3',\n",
        "                                   shape=(num_of_features, ),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.b_e = self.add_weight(name='b_e',\n",
        "                                   shape=(1, num_of_timesteps, num_of_timesteps),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_e = self.add_weight(name='V_e',\n",
        "                                   shape=(num_of_timesteps, num_of_timesteps),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        super(TemporalAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        _, T, V, F = x.shape\n",
        "\n",
        "        # shape of lhs is (batch_size, V, T)\n",
        "        lhs = K.dot(tf.transpose(x, perm=[0,1,3,2]), self.U_1)\n",
        "        lhs = tf.reshape(lhs, [tf.shape(x)[0], T, F])\n",
        "        lhs = K.dot(lhs, self.U_2)\n",
        "\n",
        "        # shape of rhs is (batch_size, T, V)\n",
        "        rhs = K.dot(self.U_3, tf.transpose(x,perm=[2,0,3,1]))\n",
        "        rhs = tf.transpose(rhs, perm=[1,0,2])\n",
        "\n",
        "        # shape of product is (batch_size, V, V)\n",
        "        product = K.batch_dot(lhs, rhs)\n",
        "\n",
        "        S = tf.transpose(K.dot(self.V_e, tf.transpose(K.sigmoid(product + self.b_e),perm=[1, 2, 0])),perm=[2, 0, 1])\n",
        "\n",
        "        # normalization\n",
        "        S = S - K.max(S, axis = 1, keepdims = True)\n",
        "        exp = K.exp(S)\n",
        "        S_normalized = exp / K.sum(exp, axis = 1, keepdims = True)\n",
        "        return S_normalized\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], input_shape[1])\n",
        "\n",
        "\n",
        "class SpatialAttention(Layer):\n",
        "    '''\n",
        "    compute spatial attention scores\n",
        "    --------\n",
        "    Input:  (batch_size, num_of_timesteps, num_of_vertices, num_of_features)\n",
        "    Output: (batch_size, num_of_vertices, num_of_vertices)\n",
        "    '''\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SpatialAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_of_timesteps, num_of_vertices, num_of_features = input_shape\n",
        "        self.W_1 = self.add_weight(name='W_1',\n",
        "                                   shape=(num_of_timesteps, 1),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.W_2 = self.add_weight(name='W_2',\n",
        "                                   shape=(num_of_features, num_of_timesteps),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.W_3 = self.add_weight(name='W_3',\n",
        "                                   shape=(num_of_features, ),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.b_s = self.add_weight(name='b_s',\n",
        "                                   shape=(1, num_of_vertices, num_of_vertices),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_s = self.add_weight(name='V_s',\n",
        "                                   shape=(num_of_vertices, num_of_vertices),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        super(SpatialAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        _, T, V, F = x.shape\n",
        "\n",
        "        # shape of lhs is (batch_size, V, T)\n",
        "        lhs = K.dot(tf.transpose(x, perm=[0,2,3,1]), self.W_1)\n",
        "        lhs = tf.reshape(lhs,[tf.shape(x)[0], V, F])\n",
        "        lhs = K.dot(lhs, self.W_2)\n",
        "\n",
        "        # shape of rhs is (batch_size, T, V)\n",
        "        rhs = K.dot(self.W_3, tf.transpose(x, perm=[1,0,3,2]))\n",
        "        rhs = tf.transpose(rhs, perm=[1,0,2])\n",
        "\n",
        "        # shape of product is (batch_size, V, V)\n",
        "        product = K.batch_dot(lhs, rhs)\n",
        "\n",
        "        S = tf.transpose(K.dot(self.V_s, tf.transpose(K.sigmoid(product + self.b_s),perm=[1, 2, 0])),perm=[2, 0, 1])\n",
        "\n",
        "        # normalization\n",
        "        S = S - K.max(S, axis = 1, keepdims = True)\n",
        "        exp = K.exp(S)\n",
        "        S_normalized = exp / K.sum(exp, axis = 1, keepdims = True)\n",
        "        return S_normalized\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[2], input_shape[2])\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "################################################################################################\n",
        "# Adaptive Graph Learning Layer\n",
        "\n",
        "def diff_loss(diff, S):\n",
        "    '''\n",
        "    compute the 1st loss of L_{graph_learning}\n",
        "    '''\n",
        "    if len(S.shape) == 4:\n",
        "        # batch input\n",
        "        return K.mean(K.sum(K.sum(diff**2, axis=3) * S, axis=(1, 2)))\n",
        "    else:\n",
        "        return K.sum(K.sum(diff**2, axis=2) * S)\n",
        "\n",
        "\n",
        "def F_norm_loss(S, Falpha):\n",
        "    '''\n",
        "    compute the 2nd loss of L_{graph_learning}\n",
        "    '''\n",
        "    if len(S.shape) == 4:\n",
        "        # batch input\n",
        "        return Falpha * K.sum(K.mean(S**2, axis=0))\n",
        "    else:\n",
        "        return Falpha * K.sum(S**2)\n",
        "\n",
        "\n",
        "class Graph_Learn(Layer):\n",
        "    '''\n",
        "    Graph structure learning (based on the middle time slice)\n",
        "    --------\n",
        "    Input:  (batch_size, num_of_timesteps, num_of_vertices, num_of_features)\n",
        "    Output: (batch_size, num_of_vertices, num_of_vertices)\n",
        "    '''\n",
        "    def __init__(self, alpha, **kwargs):\n",
        "        self.alpha = alpha\n",
        "        self.S = tf.convert_to_tensor([[[0.0]]])  # similar to placeholder\n",
        "        self.diff = tf.convert_to_tensor([[[[0.0]]]])  # similar to placeholder\n",
        "        super(Graph_Learn, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_of_timesteps, num_of_vertices, num_of_features = input_shape\n",
        "        self.a = self.add_weight(name='a',\n",
        "                                 shape=(num_of_features, 1),\n",
        "                                 initializer='uniform',\n",
        "                                 trainable=True)\n",
        "        # add loss L_{graph_learning} in the layer\n",
        "        self.add_loss(F_norm_loss(self.S, self.alpha))\n",
        "        self.add_loss(diff_loss(self.diff, self.S))\n",
        "        super(Graph_Learn, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        _, T, V, F = x.shape\n",
        "        N = tf.shape(x)[0]\n",
        "\n",
        "        outputs = []\n",
        "        diff_tmp = 0\n",
        "        for time_step in range(T):\n",
        "            # shape: (N,V,F) use the current slice\n",
        "            xt = x[:, time_step, :, :]\n",
        "            # shape: (N,V,V)\n",
        "            diff = tf.transpose(tf.broadcast_to(xt, [V,N,V,F]), perm=[2,1,0,3]) - xt\n",
        "            # shape: (N,V,V)\n",
        "            tmpS = K.exp(K.reshape(K.dot(tf.transpose(K.abs(diff), perm=[1,0,2,3]), self.a), [N,V,V]))\n",
        "            # normalization\n",
        "            S = tmpS / tf.transpose(tf.broadcast_to(K.sum(tmpS, axis=1), [V,N,V]), perm=[1,2,0])\n",
        "\n",
        "            diff_tmp += K.abs(diff)\n",
        "            outputs.append(S)\n",
        "\n",
        "        outputs = tf.transpose(outputs, perm=[1,0,2,3])\n",
        "        self.S = K.mean(outputs, axis=0)\n",
        "        self.diff = K.mean(diff_tmp, axis=0) /tf.convert_to_tensor(int(T), tf.float32)\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # shape: (n, num_of_vertices,num_of_vertices, num_of_vertices)\n",
        "        return (input_shape[0],input_shape[1],input_shape[2],input_shape[2])\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "################################################################################################\n",
        "# GCN layers\n",
        "\n",
        "class cheb_conv_with_Att_GL(Layer):\n",
        "    '''\n",
        "    K-order chebyshev graph convolution with attention after Graph Learn\n",
        "    --------\n",
        "    Input:  [x   (batch_size, num_of_timesteps, num_of_vertices, num_of_features),\n",
        "             Att (batch_size, num_of_vertices, num_of_vertices),\n",
        "             S   (batch_size, num_of_vertices, num_of_vertices)]\n",
        "    Output: (batch_size, num_of_timesteps, num_of_vertices, num_of_filters)\n",
        "    '''\n",
        "    def __init__(self, num_of_filters, k, **kwargs):\n",
        "        self.k = k\n",
        "        self.num_of_filters = num_of_filters\n",
        "        super(cheb_conv_with_Att_GL, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        x_shape, Att_shape, S_shape = input_shape\n",
        "        _, T, V, F = x_shape\n",
        "        self.Theta = self.add_weight(name='Theta',\n",
        "                                     shape=(self.k, F, self.num_of_filters),\n",
        "                                     initializer='uniform',\n",
        "                                     trainable=True)\n",
        "        super(cheb_conv_with_Att_GL, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        #Input:  [x, Att, S]\n",
        "        assert isinstance(x, list)\n",
        "        assert len(x)==3, 'Cheb_gcn input error'\n",
        "        x, Att, S = x\n",
        "        _, T, V, F = x.shape\n",
        "\n",
        "        S = K.minimum(S, tf.transpose(S,perm=[0,1,3,2])) # Ensure symmetry\n",
        "\n",
        "        # GCN\n",
        "        outputs=[]\n",
        "        for time_step in range(T):\n",
        "            # shape of x is (batch_size, V, F)\n",
        "            graph_signal = x[:, time_step, :, :]\n",
        "            output = K.zeros(shape=(tf.shape(x)[0], V, self.num_of_filters))\n",
        "\n",
        "            A = S[:, time_step, :, :]\n",
        "            #Calculating Chebyshev polynomials (let lambda_max=2)\n",
        "            D = tf.matrix_diag(K.sum(A, axis=1))\n",
        "            L = D - A\n",
        "            L_t = L - [tf.eye(int(V))]\n",
        "            cheb_polynomials = [tf.eye(int(V)), L_t]\n",
        "            for i in range(2, self.k):\n",
        "                cheb_polynomials.append(2 * L_t * cheb_polynomials[i - 1] - cheb_polynomials[i - 2])\n",
        "\n",
        "            for kk in range(self.k):\n",
        "                T_k = cheb_polynomials[kk]              # shape of T_k is (V, V)\n",
        "                T_k_with_at = T_k * Att                 # shape of T_k_with_at is (batch_size, V, V)\n",
        "                theta_k = self.Theta[kk]                # shape of theta_k is (F, num_of_filters)\n",
        "\n",
        "                # shape is (batch_size, V, F)\n",
        "                rhs = K.batch_dot(tf.transpose(T_k_with_at, perm=[0, 2, 1]), graph_signal)\n",
        "                output = output + K.dot(rhs, theta_k)\n",
        "            outputs.append(tf.expand_dims(output,-1))\n",
        "\n",
        "        return tf.transpose(K.relu(K.concatenate(outputs, axis=-1)), perm=[0,3,1,2])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # shape: (n, num_of_timesteps, num_of_vertices, num_of_filters)\n",
        "        return (input_shape[0][0], input_shape[0][1], input_shape[0][2], self.num_of_filters)\n",
        "\n",
        "\n",
        "class cheb_conv_with_Att_static(Layer):\n",
        "    '''\n",
        "    K-order chebyshev graph convolution with static graph structure\n",
        "    --------\n",
        "    Input:  [x   (batch_size, num_of_timesteps, num_of_vertices, num_of_features),\n",
        "             Att (batch_size, num_of_vertices, num_of_vertices)]\n",
        "    Output: (batch_size, num_of_timesteps, num_of_vertices, num_of_filters)\n",
        "    '''\n",
        "    def __init__(self, num_of_filters, k, cheb_polynomials, **kwargs):\n",
        "        self.k = k\n",
        "        self.num_of_filters = num_of_filters\n",
        "        self.cheb_polynomials = tf.to_float(cheb_polynomials)\n",
        "        super(cheb_conv_with_Att_static, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        x_shape, Att_shape = input_shape\n",
        "        _, T, V, F = x_shape\n",
        "        self.Theta = self.add_weight(name='Theta',\n",
        "                                     shape=(self.k, F, self.num_of_filters),\n",
        "                                     initializer='uniform',\n",
        "                                     trainable=True)\n",
        "        super(cheb_conv_with_Att_static, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        #Input:  [x, Att]\n",
        "        assert isinstance(x, list)\n",
        "        assert len(x) == 2, 'cheb_gcn error'\n",
        "        x, Att = x\n",
        "        _, T, V, F = x.shape\n",
        "\n",
        "        outputs = []\n",
        "        for time_step in range(T):\n",
        "            # shape is (batch_size, V, F)\n",
        "            graph_signal = x[:, time_step, :, :]\n",
        "            output = K.zeros(shape=(tf.shape(x)[0], V, self.num_of_filters))\n",
        "\n",
        "            for kk in range(self.k):\n",
        "                T_k = self.cheb_polynomials[kk]          # shape of T_k is (V, V)\n",
        "                T_k_with_at = K.dropout(T_k * Att, 0.6)  # shape of T_k_with_at is (batch_size, V, V)\n",
        "                theta_k = self.Theta[kk]                 # shape of theta_k is (F, num_of_filters)\n",
        "\n",
        "                # shape is (batch_size, V, F)\n",
        "                rhs = K.batch_dot(tf.transpose(T_k_with_at, perm=[0, 2, 1]), graph_signal)\n",
        "                output = output + K.dot(rhs, theta_k)\n",
        "            outputs.append(tf.expand_dims(output, -1))\n",
        "\n",
        "        return tf.transpose(K.relu(K.concatenate(outputs, axis=-1)), perm=[0, 3, 1, 2])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # shape: (n, num_of_timesteps, num_of_vertices, num_of_filters)\n",
        "        return (input_shape[0][0], input_shape[0][1], input_shape[0][2], self.num_of_filters)\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "################################################################################################\n",
        "# Some operations\n",
        "\n",
        "def reshape_dot(x):\n",
        "    #Input:  [x,TAtt]\n",
        "    x, TAtt = x\n",
        "    return tf.reshape(\n",
        "        K.batch_dot(\n",
        "            tf.reshape(tf.transpose(x, perm=[0, 2, 3, 1]),\n",
        "                       (tf.shape(x)[0], -1, tf.shape(x)[1])), TAtt),\n",
        "        [-1, x.shape[1], x.shape[2], x.shape[3]]\n",
        "    )\n",
        "\n",
        "\n",
        "def LayerNorm(x):\n",
        "    # do the layer normalization\n",
        "    relu_x = K.relu(x)\n",
        "    ln = tf.contrib.layers.layer_norm(relu_x, begin_norm_axis=3)\n",
        "    return ln\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "################################################################################################\n",
        "# Gradient Reverse Layer\n",
        "\n",
        "def reverse_gradient(X, hp_lambda):\n",
        "    \"\"\"Flips the sign of the incoming gradient during training.\"\"\"\n",
        "    num_calls=1\n",
        "    try:\n",
        "        reverse_gradient.num_calls =reverse_gradient.num_calls+ 1\n",
        "    except AttributeError:\n",
        "        reverse_gradient.num_calls = num_calls\n",
        "        num_calls=num_calls+1\n",
        "\n",
        "    grad_name = \"GradientReversal_%d\" % reverse_gradient.num_calls\n",
        "\n",
        "    @ops.RegisterGradient(grad_name)\n",
        "    def _flip_gradients(op,grad):\n",
        "        return [tf.negative(grad) * hp_lambda]\n",
        "\n",
        "    g = K.get_session().graph\n",
        "    with g.gradient_override_map({'Identity': grad_name}):\n",
        "        y = tf.identity(X)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "class GradientReversal(Layer):\n",
        "    \"\"\"Layer that flips the sign of gradient during training.\"\"\"\n",
        "\n",
        "    def __init__(self, hp_lambda, **kwargs):\n",
        "        super(GradientReversal, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.hp_lambda = hp_lambda\n",
        "\n",
        "    @staticmethod\n",
        "    def get_output_shape_for(input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.trainable_weights = []\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        return reverse_gradient(x, self.hp_lambda)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {}\n",
        "        base_config = super(GradientReversal, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "################################################################################################\n",
        "# MSTGCN Block\n",
        "\n",
        "def MSTGCN_Block(x, k, num_of_chev_filters, num_of_time_filters, time_conv_strides,\n",
        "                 cheb_polynomials, time_conv_kernel, GLalpha, i=0):\n",
        "    '''\n",
        "    packaged Spatial-temporal convolution Block\n",
        "    -------\n",
        "    x: input data;\n",
        "    k: k-order cheb GCN\n",
        "    i: block number\n",
        "    '''\n",
        "\n",
        "    # temporal attention\n",
        "    temporal_Att = TemporalAttention()(x)\n",
        "    x_TAtt = Lambda(reshape_dot, name='reshape_dot'+str(i))([x, temporal_Att])\n",
        "\n",
        "    # spatial attention\n",
        "    spatial_Att = SpatialAttention()(x_TAtt)\n",
        "\n",
        "    # multi-view GCN\n",
        "    S = Graph_Learn(alpha=GLalpha)(x)\n",
        "    S = Dropout(0.3)(S)\n",
        "    spatial_gcn_GL = cheb_conv_with_Att_GL(num_of_filters=num_of_chev_filters, k=k)([x, spatial_Att, S])\n",
        "    spatial_gcn_SD = cheb_conv_with_Att_static(num_of_filters=num_of_chev_filters, k=k,\n",
        "                                               cheb_polynomials=cheb_polynomials)([x, spatial_Att])\n",
        "\n",
        "    # temporal convolution\n",
        "    time_conv_output_GL = layers.Conv2D(\n",
        "        filters=num_of_time_filters,\n",
        "        kernel_size=(time_conv_kernel, 1),\n",
        "        padding='same',\n",
        "        strides=(1, time_conv_strides))(spatial_gcn_GL)\n",
        "\n",
        "    time_conv_output_SD = layers.Conv2D(\n",
        "        filters=num_of_time_filters,\n",
        "        kernel_size=(time_conv_kernel, 1),\n",
        "        padding='same',\n",
        "        strides=(1, time_conv_strides))(spatial_gcn_SD)\n",
        "\n",
        "    # LayerNorm\n",
        "    end_output_GL = Lambda(LayerNorm, name='layer_norm' + str(2*i))(time_conv_output_GL)\n",
        "    end_output_SD = Lambda(LayerNorm, name='layer_norm' + str(2*i+1))(time_conv_output_SD)\n",
        "    return end_output_GL, end_output_SD\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "################################################################################################\n",
        "# MSTGCN\n",
        "\n",
        "def build_MSTGCN(k, num_of_chev_filters, num_of_time_filters, time_conv_strides, cheb_polynomials,\n",
        "                 time_conv_kernel, sample_shape, num_block, dense_size, opt, GLalpha,\n",
        "                 regularizer, dropout, lambda_reversal, num_classes=5, num_domain=9):\n",
        "\n",
        "    # Input:  (*, num_of_timesteps, num_of_vertices, num_of_features)\n",
        "    data_layer = layers.Input(shape=sample_shape, name='Input_Layer')\n",
        "\n",
        "    # MSTGCN_Block\n",
        "    block_out_GL, block_out_SD = MSTGCN_Block(data_layer, k, num_of_chev_filters, num_of_time_filters,\n",
        "                                              time_conv_strides, cheb_polynomials, time_conv_kernel, GLalpha)\n",
        "    for i in range(1, num_block):\n",
        "        block_out_GL, block_out_SD = MSTGCN_Block(block_out_GL, k, num_of_chev_filters, num_of_time_filters,\n",
        "                                                  time_conv_strides, cheb_polynomials, time_conv_kernel, GLalpha, i)\n",
        "    block_out = layers.concatenate([block_out_GL, block_out_SD])\n",
        "    block_out = layers.Flatten()(block_out)\n",
        "\n",
        "    # dropout\n",
        "    if dropout != 0:\n",
        "        block_out = layers.Dropout(dropout)(block_out)\n",
        "\n",
        "    # Global dense layer\n",
        "    for size in dense_size:\n",
        "        dense_out = layers.Dense(size)(block_out)\n",
        "\n",
        "    # softmax classification\n",
        "    softmax = layers.Dense(num_classes,\n",
        "                           activation='softmax',\n",
        "                           kernel_regularizer=regularizer,\n",
        "                           name='Label')(dense_out)\n",
        "\n",
        "    # GRL & G_d\n",
        "    flip_layer = GradientReversal(lambda_reversal)\n",
        "    G_d_in = flip_layer(block_out)\n",
        "    for size in dense_size:\n",
        "        G_d_out = layers.Dense(size)(G_d_in)\n",
        "    G_d_out = layers.Dense(units=num_domain,\n",
        "                           activation='softmax',\n",
        "                           name='Domain')(G_d_out)\n",
        "\n",
        "    # training model (with GRL & G_d)\n",
        "    model = models.Model(inputs=data_layer, outputs=[softmax, G_d_out])\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['acc'],\n",
        "    )\n",
        "    # testing model (without GRL & G_d)\n",
        "    pre_model = models.Model(inputs=data_layer, outputs=softmax)\n",
        "    pre_model.compile(\n",
        "        optimizer=opt,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['acc'],\n",
        "    )\n",
        "    return model, pre_model\n",
        "\n",
        "\n",
        "def build_MSTGCN_test():\n",
        "    # an example to test\n",
        "    cheb_k = 3\n",
        "    num_of_chev_filters = 10\n",
        "    num_of_time_filters = 10\n",
        "    time_conv_strides = 1\n",
        "    time_conv_kernel = 3\n",
        "    dense_size = np.array([64, 32])\n",
        "    cheb_polynomials = [np.random.rand(26, 26), np.random.rand(26, 26), np.random.rand(26, 26)]\n",
        "\n",
        "    model = build_MSTGCN(cheb_k, num_of_chev_filters, num_of_time_filters, time_conv_strides, cheb_polynomials,\n",
        "                         time_conv_kernel, sample_shape=(5, 26, 9), num_block=1, dense_size=dense_size,\n",
        "                         opt='adam', useGL=True, GLalpha=0.0001, regularizer=None, dropout=0.0)\n",
        "    model.summary()\n",
        "    model.save('MSTGCN_build_test.h5')\n",
        "    print(\"save ok\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "vz5_V9x1G2dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train MSTGCN dependencies"
      ],
      "metadata": {
        "id": "9oGu7G7Dhacd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "import shutil\n",
        "import gc\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import keras.backend.tensorflow_backend as KTF\n",
        "\n",
        "from model.MSTGCN import build_MSTGCN\n",
        "from model.DataGenerator import DominGenerator\n",
        "from model.Utils import *"
      ],
      "metadata": {
        "id": "WFkA6LqThj5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train MSTGCN"
      ],
      "metadata": {
        "id": "rBcL219EhmOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(128 * '#')\n",
        "print('Start to train MSTGCN.')\n",
        "\n",
        "# # 1. Get configuration\n",
        "\n",
        "# ## 1.1. Read .config file\n",
        "\n",
        "# command line parameters -c -g\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"-c\", type = str, help = \"configuration file\", required = True)\n",
        "parser.add_argument(\"-g\", type = str, help = \"GPU number to use, set '-1' to use CPU\", required = True)\n",
        "args = parser.parse_args()\n",
        "Path, _, cfgTrain, cfgModel = ReadConfig(args.c)\n",
        "\n",
        "# set GPU number or use CPU only\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.g\n",
        "if args.g != \"-1\":\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth=True\n",
        "    sess = tf.Session(config=config)\n",
        "    KTF.set_session(sess)\n",
        "    print(\"Use GPU #\"+args.g)\n",
        "else:\n",
        "    print(\"Use CPU only\")\n",
        "\n",
        "# ## 1.2. Analytic parameters\n",
        "\n",
        "# [train] parameters ('_f' means FeatureNet)\n",
        "channels   = int(cfgTrain[\"channels\"])\n",
        "fold       = int(cfgTrain[\"fold\"])\n",
        "context    = int(cfgTrain[\"context\"])\n",
        "num_epochs = int(cfgTrain[\"epoch\"])\n",
        "batch_size = int(cfgTrain[\"batch_size\"])\n",
        "optimizer  = cfgTrain[\"optimizer\"]\n",
        "learn_rate = float(cfgTrain[\"learn_rate\"])\n",
        "lambda_GRL   = float(cfgTrain[\"lambda_GRL\"])\n",
        "\n",
        "# [model] parameters\n",
        "dense_size            = np.array(str.split(cfgModel[\"Globaldense\"],','),dtype=int)\n",
        "GLalpha               = float(cfgModel[\"GLalpha\"])\n",
        "num_of_chev_filters   = int(cfgModel[\"cheb_filters\"])\n",
        "num_of_time_filters   = int(cfgModel[\"time_filters\"])\n",
        "time_conv_strides     = int(cfgModel[\"time_conv_strides\"])\n",
        "time_conv_kernel      = int(cfgModel[\"time_conv_kernel\"])\n",
        "num_block             = int(cfgModel[\"num_block\"])\n",
        "cheb_k                = int(cfgModel[\"cheb_k\"])\n",
        "l1                    = float(cfgModel[\"l1\"])\n",
        "l2                    = float(cfgModel[\"l2\"])\n",
        "dropout               = float(cfgModel[\"dropout\"])\n",
        "\n",
        "# ## 1.3. Parameter check and enable\n",
        "\n",
        "# Create save pathand copy .config to it\n",
        "if not os.path.exists(Path['Save']):\n",
        "    os.makedirs(Path['Save'])\n",
        "shutil.copyfile(args.c, Path['Save']+\"last.config\")\n",
        "\n",
        "\n",
        "# # 2. Read data and process data\n",
        "\n",
        "# ## 2.1. Read data\n",
        "# Each fold corresponds to one subject's data (ISRUC-S3 dataset)\n",
        "ReadList = np.load(Path['data'], allow_pickle=True)\n",
        "Fold_Num   = ReadList['Fold_len']    # Num of samples of each fold\n",
        "\n",
        "# ## 2.2. Read adjacency matrix\n",
        "# Prepare Chebyshev polynomial of G_DC\n",
        "Dis_Conn = np.load(Path['disM'], allow_pickle=True)  # shape:[V,V]\n",
        "L_DC = scaled_Laplacian(Dis_Conn)                    # Calculate laplacian matrix\n",
        "cheb_poly_DC = cheb_polynomial(L_DC, cheb_k)         # K-order Chebyshev polynomial\n",
        "\n",
        "print(\"Read data successfully\")\n",
        "Fold_Num_c  = Fold_Num + 1 - context\n",
        "print('Number of samples: ',np.sum(Fold_Num), '(with context:', np.sum(Fold_Num_c), ')')\n",
        "\n",
        "# ## 2.3. Build kFoldGenerator or DominGenerator\n",
        "Dom_Generator = DominGenerator(Fold_Num_c)\n",
        "\n",
        "\n",
        "# # 3. Model training (cross validation)\n",
        "\n",
        "# k-fold cross validation\n",
        "all_scores = []\n",
        "for i in range(fold):\n",
        "    print(128*'_')\n",
        "    print('Fold #', i)\n",
        "\n",
        "    # Instantiation optimizer\n",
        "    opt = Instantiation_optim(optimizer, learn_rate)\n",
        "    # Instantiation l1, l2 regularizer\n",
        "    regularizer = Instantiation_regularizer(l1, l2)\n",
        "\n",
        "    # get i th-fold feature and label\n",
        "    Features = np.load(Path['Save']+'Feature_'+str(i)+'.npz', allow_pickle=True)\n",
        "    train_feature = Features['train_feature']\n",
        "    val_feature   = Features['val_feature']\n",
        "    train_targets = Features['train_targets']\n",
        "    val_targets   = Features['val_targets']\n",
        "\n",
        "    ## Use the feature to train MSTGCN\n",
        "\n",
        "    print('Feature',train_feature.shape,val_feature.shape)\n",
        "    train_feature, train_targets  = AddContext_MultiSub(train_feature, train_targets,\n",
        "                                                        np.delete(Fold_Num.copy(), i), context, i)\n",
        "    val_feature, val_targets      = AddContext_SingleSub(val_feature, val_targets, context)\n",
        "    train_domin, val_domin = Dom_Generator.getFold(i)\n",
        "\n",
        "    sample_shape = (val_feature.shape[1:])\n",
        "\n",
        "    print('Feature with context:',train_feature.shape, val_feature.shape)\n",
        "    model, model_p = build_MSTGCN(cheb_k, num_of_chev_filters, num_of_time_filters, time_conv_strides, cheb_poly_DC,\n",
        "                                  time_conv_kernel, sample_shape, num_block, dense_size, opt, GLalpha, regularizer,\n",
        "                                  dropout, lambda_GRL, num_classes=5, num_domain=9) # '_p' model is without GRL\n",
        "\n",
        "    # train\n",
        "    history = model.fit(\n",
        "        x = train_feature,\n",
        "        y = [train_targets,train_domin],\n",
        "        epochs = num_epochs,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = True,\n",
        "        validation_data = (val_feature, [val_targets,val_domin]),\n",
        "        verbose = 2,\n",
        "        callbacks=[keras.callbacks.ModelCheckpoint(Path['Save']+'MSTGCN_Best_'+str(i)+'.h5',\n",
        "                                                   monitor='val_Label_acc',\n",
        "                                                   verbose=0,\n",
        "                                                   save_best_only=True,\n",
        "                                                   save_weights_only=False,\n",
        "                                                   mode='auto',\n",
        "                                                   period=1 )])\n",
        "\n",
        "    # save the final model\n",
        "    model.save(Path['Save']+'MSTGCN_Final_'+str(i)+'.h5')\n",
        "\n",
        "    # Save training information\n",
        "    if i==0:\n",
        "        fit_loss = np.array(history.history['loss'])*Fold_Num_c[i]\n",
        "        fit_acc = np.array(history.history['Label_acc'])*Fold_Num_c[i]\n",
        "        fit_val_loss = np.array(history.history['val_loss'])*Fold_Num_c[i]\n",
        "        fit_val_acc = np.array(history.history['val_Label_acc'])*Fold_Num_c[i]\n",
        "    else:\n",
        "        fit_loss = fit_loss+np.array(history.history['loss'])*Fold_Num_c[i]\n",
        "        fit_acc = fit_acc+np.array(history.history['Label_acc'])*Fold_Num_c[i]\n",
        "        fit_val_loss = fit_val_loss+np.array(history.history['val_loss'])*Fold_Num_c[i]\n",
        "        fit_val_acc = fit_val_acc+np.array(history.history['val_Label_acc'])*Fold_Num_c[i]\n",
        "\n",
        "    saveFile = open(Path['Save'] + \"Result_MSTGCN.txt\", 'a+')\n",
        "    print('Fold #'+str(i), file=saveFile)\n",
        "    print(history.history, file=saveFile)\n",
        "    saveFile.close()\n",
        "\n",
        "    # Fold finish\n",
        "    keras.backend.clear_session()\n",
        "    del model, model_p, train_feature, train_targets, val_feature, val_targets\n",
        "    gc.collect()\n",
        "\n",
        "# # 4. Final results\n",
        "\n",
        "# Average training performance\n",
        "fit_acc      = fit_acc/np.sum(Fold_Num_c)\n",
        "fit_loss     = fit_loss/np.sum(Fold_Num_c)\n",
        "fit_val_loss = fit_val_loss/np.sum(Fold_Num_c)\n",
        "fit_val_acc  = fit_val_acc/np.sum(Fold_Num_c)\n",
        "\n",
        "# Draw ACC / loss curve and save\n",
        "VariationCurve(fit_acc, fit_val_acc, 'Acc', Path['Save'], figsize=(9, 6))\n",
        "VariationCurve(fit_loss, fit_val_loss, 'Loss', Path['Save'], figsize=(9, 6))\n",
        "\n",
        "saveFile = open(Path['Save'] + \"Result_MSTGCN.txt\", 'a+')\n",
        "print(history.history, file=saveFile)\n",
        "saveFile.close()\n",
        "\n",
        "print(128 * '_')\n",
        "print('End of training MSTGCN.')\n",
        "print(128 * '#')\n"
      ],
      "metadata": {
        "id": "h8PzpWYwGxcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}